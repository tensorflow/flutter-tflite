// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
import 'dart:ffi' as ffi;

/// Bindings for `src/tensorflow_lite/c_api.h`.
///
/// Regenerate bindings with `flutter pub run ffigen --config ffigen_tensorflow_lite.yaml`.
///
class TensorFlowLiteBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
      _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  TensorFlowLiteBindings(ffi.DynamicLibrary dynamicLibrary)
      : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  TensorFlowLiteBindings.fromLookup(
      ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
          lookup)
      : _lookup = lookup;

  /// --------------------------------------------------------------------------
  /// TfLiteVersion returns a string describing version information of the
  /// TensorFlow Lite library. TensorFlow Lite uses semantic versioning.
  ffi.Pointer<ffi.Char> TfLiteVersion() {
    return _TfLiteVersion();
  }

  late final _TfLiteVersionPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
          'TfLiteVersion');
  late final _TfLiteVersion =
      _TfLiteVersionPtr.asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// Returns a model from the provided buffer, or null on failure.
  ///
  /// NOTE: The caller retains ownership of the `model_data` buffer and should
  /// ensure that the lifetime of the `model_data` buffer must be at least as long
  /// as the lifetime of the `TfLiteModel` and of any `TfLiteInterpreter` objects
  /// created from that `TfLiteModel`, and furthermore the contents of the
  /// `model_data` buffer must not be modified during that time."
  ffi.Pointer<TfLiteModel> TfLiteModelCreate(
    ffi.Pointer<ffi.Void> model_data,
    int model_size,
  ) {
    return _TfLiteModelCreate(
      model_data,
      model_size,
    );
  }

  late final _TfLiteModelCreatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Void>, ffi.Size)>>('TfLiteModelCreate');
  late final _TfLiteModelCreate = _TfLiteModelCreatePtr.asFunction<
      ffi.Pointer<TfLiteModel> Function(ffi.Pointer<ffi.Void>, int)>();

  /// Returns a model from the provided file, or null on failure.
  ///
  /// NOTE: The file's contents must not be modified during the lifetime of the
  /// `TfLiteModel` or of any `TfLiteInterpreter` objects created from that
  /// `TfLiteModel`.
  ffi.Pointer<TfLiteModel> TfLiteModelCreateFromFile(
    ffi.Pointer<ffi.Char> model_path,
  ) {
    return _TfLiteModelCreateFromFile(
      model_path,
    );
  }

  late final _TfLiteModelCreateFromFilePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Char>)>>('TfLiteModelCreateFromFile');
  late final _TfLiteModelCreateFromFile = _TfLiteModelCreateFromFilePtr
      .asFunction<ffi.Pointer<TfLiteModel> Function(ffi.Pointer<ffi.Char>)>();

  /// Destroys the model instance.
  void TfLiteModelDelete(
    ffi.Pointer<TfLiteModel> model,
  ) {
    return _TfLiteModelDelete(
      model,
    );
  }

  late final _TfLiteModelDeletePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteModel>)>>(
          'TfLiteModelDelete');
  late final _TfLiteModelDelete = _TfLiteModelDeletePtr.asFunction<
      void Function(ffi.Pointer<TfLiteModel>)>();

  /// Returns a new TfLiteRegistrationExternal instance.
  ///
  /// NOTE: The caller retains ownership and should ensure that
  /// the lifetime of the `TfLiteRegistrationExternal` must be at least as long as
  /// the lifetime of the `TfLiteInterpreter`.
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<TfLiteRegistrationExternal> TfLiteRegistrationExternalCreate(
    int builtin_code,
    ffi.Pointer<ffi.Char> custom_name,
    int version,
  ) {
    return _TfLiteRegistrationExternalCreate(
      builtin_code,
      custom_name,
      version,
    );
  }

  late final _TfLiteRegistrationExternalCreatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteRegistrationExternal> Function(
              ffi.Int32,
              ffi.Pointer<ffi.Char>,
              ffi.Int)>>('TfLiteRegistrationExternalCreate');
  late final _TfLiteRegistrationExternalCreate =
      _TfLiteRegistrationExternalCreatePtr.asFunction<
          ffi.Pointer<TfLiteRegistrationExternal> Function(
              int, ffi.Pointer<ffi.Char>, int)>();

  /// Return the builtin op code of the provided external 'registration'.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteRegistrationExternalGetBuiltInCode(
    ffi.Pointer<TfLiteRegistrationExternal> registration,
  ) {
    return _TfLiteRegistrationExternalGetBuiltInCode(
      registration,
    );
  }

  late final _TfLiteRegistrationExternalGetBuiltInCodePtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<TfLiteRegistrationExternal>)>>(
      'TfLiteRegistrationExternalGetBuiltInCode');
  late final _TfLiteRegistrationExternalGetBuiltInCode =
      _TfLiteRegistrationExternalGetBuiltInCodePtr.asFunction<
          int Function(ffi.Pointer<TfLiteRegistrationExternal>)>();

  /// Destroys the TfLiteRegistrationExternal instance.
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteRegistrationExternalDelete(
    ffi.Pointer<TfLiteRegistrationExternal> registration,
  ) {
    return _TfLiteRegistrationExternalDelete(
      registration,
    );
  }

  late final _TfLiteRegistrationExternalDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteRegistrationExternal>)>>(
      'TfLiteRegistrationExternalDelete');
  late final _TfLiteRegistrationExternalDelete =
      _TfLiteRegistrationExternalDeletePtr.asFunction<
          void Function(ffi.Pointer<TfLiteRegistrationExternal>)>();

  /// Sets the initialization callback for the registration.
  ///
  /// The callback is called to initialize the op from serialized data.
  /// Please refer `init` of `TfLiteRegistration` for the detail.
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteRegistrationExternalSetInit(
    ffi.Pointer<TfLiteRegistrationExternal> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<ffi.Void> Function(
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<ffi.Char> buffer,
                    ffi.Size length)>>
        init,
  ) {
    return _TfLiteRegistrationExternalSetInit(
      registration,
      init,
    );
  }

  late final _TfLiteRegistrationExternalSetInitPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteRegistrationExternal>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Pointer<ffi.Void> Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<ffi.Char> buffer,
                              ffi.Size length)>>)>>(
      'TfLiteRegistrationExternalSetInit');
  late final _TfLiteRegistrationExternalSetInit =
      _TfLiteRegistrationExternalSetInitPtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteRegistrationExternal>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<ffi.Void> Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<ffi.Char> buffer,
                          ffi.Size length)>>)>();

  /// Sets the deallocation callback for the registration.
  ///
  /// This callback is called to deallocate the data returned by the init callback.
  /// The value passed in the `data` parameter is the value that was returned by
  /// the `init` callback.
  /// Please refer `free` of `TfLiteRegistration` for the detail.
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteRegistrationExternalSetFree(
    ffi.Pointer<TfLiteRegistrationExternal> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<ffi.Void> data)>>
        free,
  ) {
    return _TfLiteRegistrationExternalSetFree(
      registration,
      free,
    );
  }

  late final _TfLiteRegistrationExternalSetFreePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteRegistrationExternal>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Void Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<ffi.Void> data)>>)>>(
      'TfLiteRegistrationExternalSetFree');
  late final _TfLiteRegistrationExternalSetFree =
      _TfLiteRegistrationExternalSetFreePtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteRegistrationExternal>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<ffi.Void> data)>>)>();

  /// Sets the preparation callback for the registration.
  ///
  /// The callback is called when the inputs of operator have been resized.
  /// Please refer `prepare` of `TfLiteRegistration` for the detail.
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteRegistrationExternalSetPrepare(
    ffi.Pointer<TfLiteRegistrationExternal> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Int32 Function(ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        prepare,
  ) {
    return _TfLiteRegistrationExternalSetPrepare(
      registration,
      prepare,
    );
  }

  late final _TfLiteRegistrationExternalSetPreparePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteRegistrationExternal>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Int32 Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteRegistrationExternalSetPrepare');
  late final _TfLiteRegistrationExternalSetPrepare =
      _TfLiteRegistrationExternalSetPreparePtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteRegistrationExternal>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Int32 Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the invocation callback for the registration.
  ///
  /// The callback is called when the operator is executed.
  /// Please refer `invoke` of `TfLiteRegistration` for the detail.
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteRegistrationExternalSetInvoke(
    ffi.Pointer<TfLiteRegistrationExternal> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Int32 Function(ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        invoke,
  ) {
    return _TfLiteRegistrationExternalSetInvoke(
      registration,
      invoke,
    );
  }

  late final _TfLiteRegistrationExternalSetInvokePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteRegistrationExternal>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Int32 Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteRegistrationExternalSetInvoke');
  late final _TfLiteRegistrationExternalSetInvoke =
      _TfLiteRegistrationExternalSetInvokePtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteRegistrationExternal>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Int32 Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Returns a new interpreter options instances.
  ffi.Pointer<TfLiteInterpreterOptions> TfLiteInterpreterOptionsCreate() {
    return _TfLiteInterpreterOptionsCreate();
  }

  late final _TfLiteInterpreterOptionsCreatePtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<TfLiteInterpreterOptions> Function()>>(
      'TfLiteInterpreterOptionsCreate');
  late final _TfLiteInterpreterOptionsCreate =
      _TfLiteInterpreterOptionsCreatePtr.asFunction<
          ffi.Pointer<TfLiteInterpreterOptions> Function()>();

  /// Destroys the interpreter options instance.
  void TfLiteInterpreterOptionsDelete(
    ffi.Pointer<TfLiteInterpreterOptions> options,
  ) {
    return _TfLiteInterpreterOptionsDelete(
      options,
    );
  }

  late final _TfLiteInterpreterOptionsDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>)>>(
      'TfLiteInterpreterOptionsDelete');
  late final _TfLiteInterpreterOptionsDelete =
      _TfLiteInterpreterOptionsDeletePtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>)>();

  /// Sets the number of CPU threads to use for the interpreter.
  void TfLiteInterpreterOptionsSetNumThreads(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    int num_threads,
  ) {
    return _TfLiteInterpreterOptionsSetNumThreads(
      options,
      num_threads,
    );
  }

  late final _TfLiteInterpreterOptionsSetNumThreadsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Int32)>>('TfLiteInterpreterOptionsSetNumThreads');
  late final _TfLiteInterpreterOptionsSetNumThreads =
      _TfLiteInterpreterOptionsSetNumThreadsPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>, int)>();

  /// Adds a delegate to be applied during `TfLiteInterpreter` creation.
  ///
  /// If delegate application fails, interpreter creation will also fail with an
  /// associated error logged.
  ///
  /// NOTE: The caller retains ownership of the delegate and should ensure that it
  /// remains valid for the duration of any created interpreter's lifetime.
  void TfLiteInterpreterOptionsAddDelegate(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TfLiteInterpreterOptionsAddDelegate(
      options,
      delegate,
    );
  }

  late final _TfLiteInterpreterOptionsAddDelegatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<TfLiteDelegate>)>>(
      'TfLiteInterpreterOptionsAddDelegate');
  late final _TfLiteInterpreterOptionsAddDelegate =
      _TfLiteInterpreterOptionsAddDelegatePtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<TfLiteDelegate>)>();

  /// Adds an opaque delegate to be applied during `TfLiteInterpreter` creation.
  ///
  /// If delegate application fails, interpreter creation will also fail with an
  /// associated error logged.
  ///
  /// NOTE: The caller retains ownership of the delegate and should ensure that it
  /// remains valid for the duration of any created interpreter's lifetime.
  void TfLiteInterpreterOptionsAddOpaqueDelegate(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<TfLiteOpaqueDelegateStruct> opaque_delegate,
  ) {
    return _TfLiteInterpreterOptionsAddOpaqueDelegate(
      options,
      opaque_delegate,
    );
  }

  late final _TfLiteInterpreterOptionsAddOpaqueDelegatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<TfLiteOpaqueDelegateStruct>)>>(
      'TfLiteInterpreterOptionsAddOpaqueDelegate');
  late final _TfLiteInterpreterOptionsAddOpaqueDelegate =
      _TfLiteInterpreterOptionsAddOpaqueDelegatePtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<TfLiteOpaqueDelegateStruct>)>();

  /// Sets a custom error reporter for interpreter execution.
  ///
  /// * `reporter` takes the provided `user_data` object, as well as a C-style
  /// format string and arg list (see also vprintf).
  /// * `user_data` is optional. If non-null, it is owned by the client and must
  /// remain valid for the duration of the interpreter lifetime.
  void TfLiteInterpreterOptionsSetErrorReporter(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<ffi.Char> format, va_list args)>>
        reporter,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _TfLiteInterpreterOptionsSetErrorReporter(
      options,
      reporter,
      user_data,
    );
  }

  late final _TfLiteInterpreterOptionsSetErrorReporterPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Void Function(ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<ffi.Char> format, va_list args)>>,
                  ffi.Pointer<ffi.Void>)>>(
      'TfLiteInterpreterOptionsSetErrorReporter');
  late final _TfLiteInterpreterOptionsSetErrorReporter =
      _TfLiteInterpreterOptionsSetErrorReporterPtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> format, va_list args)>>,
              ffi.Pointer<ffi.Void>)>();

  /// Adds an op registration to be applied during `TfLiteInterpreter` creation.
  ///
  /// The `TfLiteRegistrationExternal` object is needed to implement custom op of
  /// TFLite Interpreter via C API. Calling this function ensures that any
  /// `TfLiteInterpreter` created with the specified `options` can execute models
  /// that use the custom operator specified in `registration`.
  /// Please refer https://www.tensorflow.org/lite/guide/ops_custom for custom op
  /// support.
  /// NOTE: The caller retains ownership of the TfLiteRegistrationExternal object
  /// and should ensure that it remains valid for the duration of any created
  /// interpreter's lifetime.
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsAddRegistrationExternal(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<TfLiteRegistrationExternal> registration,
  ) {
    return _TfLiteInterpreterOptionsAddRegistrationExternal(
      options,
      registration,
    );
  }

  late final _TfLiteInterpreterOptionsAddRegistrationExternalPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<TfLiteRegistrationExternal>)>>(
      'TfLiteInterpreterOptionsAddRegistrationExternal');
  late final _TfLiteInterpreterOptionsAddRegistrationExternal =
      _TfLiteInterpreterOptionsAddRegistrationExternalPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<TfLiteRegistrationExternal>)>();

  /// Returns a new interpreter using the provided model and options, or null on
  /// failure.
  ///
  /// * `model` must be a valid model instance. The caller retains ownership of the
  /// object, and may destroy it (via TfLiteModelDelete) immediately after
  /// creating the interpreter.  However, if the TfLiteModel was allocated with
  /// TfLiteModelCreate, then the `model_data` buffer that was passed to
  /// TfLiteModelCreate must outlive the lifetime of the TfLiteInterpreter object
  /// that this function returns, and must not be modified during that time;
  /// and if the TfLiteModel was allocated with TfLiteModelCreateFromFile, then
  /// the contents of the model file must not be modified during the lifetime of
  /// the TfLiteInterpreter object that this function returns.
  /// * `optional_options` may be null. The caller retains ownership of the object,
  /// and can safely destroy it (via TfLiteInterpreterOptionsDelete) immediately
  /// after creating the interpreter.
  ///
  /// NOTE: The client *must* explicitly allocate tensors before attempting to
  /// access input tensor data or invoke the interpreter.
  ffi.Pointer<TfLiteInterpreter> TfLiteInterpreterCreate(
    ffi.Pointer<TfLiteModel> model,
    ffi.Pointer<TfLiteInterpreterOptions> optional_options,
  ) {
    return _TfLiteInterpreterCreate(
      model,
      optional_options,
    );
  }

  late final _TfLiteInterpreterCreatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteInterpreter> Function(ffi.Pointer<TfLiteModel>,
                  ffi.Pointer<TfLiteInterpreterOptions>)>>(
      'TfLiteInterpreterCreate');
  late final _TfLiteInterpreterCreate = _TfLiteInterpreterCreatePtr.asFunction<
      ffi.Pointer<TfLiteInterpreter> Function(
          ffi.Pointer<TfLiteModel>, ffi.Pointer<TfLiteInterpreterOptions>)>();

  /// Destroys the interpreter.
  void TfLiteInterpreterDelete(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterDelete(
      interpreter,
    );
  }

  late final _TfLiteInterpreterDeletePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterDelete');
  late final _TfLiteInterpreterDelete = _TfLiteInterpreterDeletePtr.asFunction<
      void Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the number of input tensors associated with the model.
  int TfLiteInterpreterGetInputTensorCount(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterGetInputTensorCount(
      interpreter,
    );
  }

  late final _TfLiteInterpreterGetInputTensorCountPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterGetInputTensorCount');
  late final _TfLiteInterpreterGetInputTensorCount =
      _TfLiteInterpreterGetInputTensorCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the tensor associated with the input index.
  /// REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
  ffi.Pointer<TfLiteTensor> TfLiteInterpreterGetInputTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int input_index,
  ) {
    return _TfLiteInterpreterGetInputTensor(
      interpreter,
      input_index,
    );
  }

  late final _TfLiteInterpreterGetInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetInputTensor');
  late final _TfLiteInterpreterGetInputTensor =
      _TfLiteInterpreterGetInputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Resizes the specified input tensor.
  ///
  /// NOTE: After a resize, the client *must* explicitly allocate tensors before
  /// attempting to access the resized tensor data or invoke the interpreter.
  ///
  /// REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
  ///
  /// This function makes a copy of the input dimensions, so the client can safely
  /// deallocate `input_dims` immediately after this function returns.
  int TfLiteInterpreterResizeInputTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int input_index,
    ffi.Pointer<ffi.Int> input_dims,
    int input_dims_size,
  ) {
    return _TfLiteInterpreterResizeInputTensor(
      interpreter,
      input_index,
      input_dims,
      input_dims_size,
    );
  }

  late final _TfLiteInterpreterResizeInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32,
              ffi.Pointer<ffi.Int>,
              ffi.Int32)>>('TfLiteInterpreterResizeInputTensor');
  late final _TfLiteInterpreterResizeInputTensor =
      _TfLiteInterpreterResizeInputTensorPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>, int,
              ffi.Pointer<ffi.Int>, int)>();

  /// Updates allocations for all tensors, resizing dependent tensors using the
  /// specified input tensor dimensionality.
  ///
  /// This is a relatively expensive operation, and need only be called after
  /// creating the graph and/or resizing any inputs.
  int TfLiteInterpreterAllocateTensors(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterAllocateTensors(
      interpreter,
    );
  }

  late final _TfLiteInterpreterAllocateTensorsPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterAllocateTensors');
  late final _TfLiteInterpreterAllocateTensors =
      _TfLiteInterpreterAllocateTensorsPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Runs inference for the loaded graph.
  ///
  /// Before calling this function, the caller should first invoke
  /// TfLiteInterpreterAllocateTensors() and should also set the values for the
  /// input tensors.  After successfully calling this function, the values for the
  /// output tensors will be set.
  ///
  /// NOTE: It is possible that the interpreter is not in a ready state to
  /// evaluate (e.g., if AllocateTensors() hasn't been called, or if a
  /// ResizeInputTensor() has been performed without a subsequent call to
  /// AllocateTensors()).
  ///
  /// If the (experimental!) delegate fallback option was enabled in the
  /// interpreter options, then the interpreter will automatically fall back to
  /// not using any delegates if execution with delegates fails. For details, see
  /// TfLiteInterpreterOptionsSetEnableDelegateFallback in c_api_experimental.h.
  ///
  /// Returns one of the following status codes:
  /// - kTfLiteOk: Success. Output is valid.
  /// - kTfLiteDelegateError: Execution with delegates failed, due to a problem
  /// with the delegate(s). If fallback was not enabled, output is invalid.
  /// If fallback was enabled, this return value indicates that fallback
  /// succeeded, the output is valid, and all delegates previously applied to
  /// the interpreter have been undone.
  /// - kTfLiteApplicationError: Same as for kTfLiteDelegateError, except that
  /// the problem was not with the delegate itself, but rather was
  /// due to an incompatibility between the delegate(s) and the
  /// interpreter or model.
  /// - kTfLiteError: Unexpected/runtime failure. Output is invalid.
  int TfLiteInterpreterInvoke(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterInvoke(
      interpreter,
    );
  }

  late final _TfLiteInterpreterInvokePtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterInvoke');
  late final _TfLiteInterpreterInvoke = _TfLiteInterpreterInvokePtr.asFunction<
      int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the number of output tensors associated with the model.
  int TfLiteInterpreterGetOutputTensorCount(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterGetOutputTensorCount(
      interpreter,
    );
  }

  late final _TfLiteInterpreterGetOutputTensorCountPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterGetOutputTensorCount');
  late final _TfLiteInterpreterGetOutputTensorCount =
      _TfLiteInterpreterGetOutputTensorCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the tensor associated with the output index.
  /// REQUIRES: 0 <= output_index < TfLiteInterpreterGetOutputTensorCount(tensor)
  ///
  /// NOTE: The shape and underlying data buffer for output tensors may be not
  /// be available until after the output tensor has been both sized and allocated.
  /// In general, best practice is to interact with the output tensor *after*
  /// calling TfLiteInterpreterInvoke().
  ffi.Pointer<TfLiteTensor> TfLiteInterpreterGetOutputTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int output_index,
  ) {
    return _TfLiteInterpreterGetOutputTensor(
      interpreter,
      output_index,
    );
  }

  late final _TfLiteInterpreterGetOutputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetOutputTensor');
  late final _TfLiteInterpreterGetOutputTensor =
      _TfLiteInterpreterGetOutputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Returns the type of a tensor element.
  int TfLiteTensorType(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorType(
      tensor,
    );
  }

  late final _TfLiteTensorTypePtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorType');
  late final _TfLiteTensorType = _TfLiteTensorTypePtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the number of dimensions that the tensor has.
  int TfLiteTensorNumDims(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorNumDims(
      tensor,
    );
  }

  late final _TfLiteTensorNumDimsPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorNumDims');
  late final _TfLiteTensorNumDims = _TfLiteTensorNumDimsPtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the length of the tensor in the "dim_index" dimension.
  /// REQUIRES: 0 <= dim_index < TFLiteTensorNumDims(tensor)
  int TfLiteTensorDim(
    ffi.Pointer<TfLiteTensor> tensor,
    int dim_index,
  ) {
    return _TfLiteTensorDim(
      tensor,
      dim_index,
    );
  }

  late final _TfLiteTensorDimPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteTensor>, ffi.Int32)>>('TfLiteTensorDim');
  late final _TfLiteTensorDim = _TfLiteTensorDimPtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>, int)>();

  /// Returns the size of the underlying data in bytes.
  int TfLiteTensorByteSize(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorByteSize(
      tensor,
    );
  }

  late final _TfLiteTensorByteSizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<TfLiteTensor>)>>(
          'TfLiteTensorByteSize');
  late final _TfLiteTensorByteSize = _TfLiteTensorByteSizePtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns a pointer to the underlying data buffer.
  ///
  /// NOTE: The result may be null if tensors have not yet been allocated, e.g.,
  /// if the Tensor has just been created or resized and `TfLiteAllocateTensors()`
  /// has yet to be called, or if the output tensor is dynamically sized and the
  /// interpreter hasn't been invoked.
  ffi.Pointer<ffi.Void> TfLiteTensorData(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorData(
      tensor,
    );
  }

  late final _TfLiteTensorDataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorData');
  late final _TfLiteTensorData = _TfLiteTensorDataPtr.asFunction<
      ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the (null-terminated) name of the tensor.
  ffi.Pointer<ffi.Char> TfLiteTensorName(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorName(
      tensor,
    );
  }

  late final _TfLiteTensorNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorName');
  late final _TfLiteTensorName = _TfLiteTensorNamePtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the parameters for asymmetric quantization. The quantization
  /// parameters are only valid when the tensor type is `kTfLiteUInt8` and the
  /// `scale != 0`. Quantized values can be converted back to float using:
  /// real_value = scale * (quantized_value - zero_point);
  TfLiteQuantizationParams TfLiteTensorQuantizationParams(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorQuantizationParams(
      tensor,
    );
  }

  late final _TfLiteTensorQuantizationParamsPtr = _lookup<
      ffi.NativeFunction<
          TfLiteQuantizationParams Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorQuantizationParams');
  late final _TfLiteTensorQuantizationParams =
      _TfLiteTensorQuantizationParamsPtr.asFunction<
          TfLiteQuantizationParams Function(ffi.Pointer<TfLiteTensor>)>();

  /// Copies from the provided input buffer into the tensor's buffer.
  /// REQUIRES: input_data_size == TfLiteTensorByteSize(tensor)
  int TfLiteTensorCopyFromBuffer(
    ffi.Pointer<TfLiteTensor> tensor,
    ffi.Pointer<ffi.Void> input_data,
    int input_data_size,
  ) {
    return _TfLiteTensorCopyFromBuffer(
      tensor,
      input_data,
      input_data_size,
    );
  }

  late final _TfLiteTensorCopyFromBufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteTensor>, ffi.Pointer<ffi.Void>,
              ffi.Size)>>('TfLiteTensorCopyFromBuffer');
  late final _TfLiteTensorCopyFromBuffer =
      _TfLiteTensorCopyFromBufferPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteTensor>, ffi.Pointer<ffi.Void>, int)>();

  /// Copies to the provided output buffer from the tensor's buffer.
  /// REQUIRES: output_data_size == TfLiteTensorByteSize(tensor)
  int TfLiteTensorCopyToBuffer(
    ffi.Pointer<TfLiteTensor> output_tensor,
    ffi.Pointer<ffi.Void> output_data,
    int output_data_size,
  ) {
    return _TfLiteTensorCopyToBuffer(
      output_tensor,
      output_data,
      output_data_size,
    );
  }

  late final _TfLiteTensorCopyToBufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteTensor>, ffi.Pointer<ffi.Void>,
              ffi.Size)>>('TfLiteTensorCopyToBuffer');
  late final _TfLiteTensorCopyToBuffer =
      _TfLiteTensorCopyToBufferPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteTensor>, ffi.Pointer<ffi.Void>, int)>();

  /// --------------------------------------------------------------------------
  /// Resets all variable tensors to zero.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteInterpreterResetVariableTensors(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterResetVariableTensors(
      interpreter,
    );
  }

  late final _TfLiteInterpreterResetVariableTensorsPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterResetVariableTensors');
  late final _TfLiteInterpreterResetVariableTensors =
      _TfLiteInterpreterResetVariableTensorsPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Adds an op registration for a builtin operator.
  ///
  /// Op registrations are used to map ops referenced in the flatbuffer model
  /// to executable function pointers (`TfLiteRegistration`s).
  ///
  /// NOTE: The interpreter will make a shallow copy of `registration` internally,
  /// so the caller should ensure that its contents (function pointers, etc...)
  /// remain valid for the duration of the interpreter's lifetime. A common
  /// practice is making the provided `TfLiteRegistration` instance static.
  ///
  /// Code that uses this function should NOT call
  /// `TfLiteInterpreterOptionsSetOpResolver` on the same options object.
  ///
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsAddBuiltinOp(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    int op,
    ffi.Pointer<TfLiteRegistration> registration,
    int min_version,
    int max_version,
  ) {
    return _TfLiteInterpreterOptionsAddBuiltinOp(
      options,
      op,
      registration,
      min_version,
      max_version,
    );
  }

  late final _TfLiteInterpreterOptionsAddBuiltinOpPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Int32,
              ffi.Pointer<TfLiteRegistration>,
              ffi.Int32,
              ffi.Int32)>>('TfLiteInterpreterOptionsAddBuiltinOp');
  late final _TfLiteInterpreterOptionsAddBuiltinOp =
      _TfLiteInterpreterOptionsAddBuiltinOpPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>, int,
              ffi.Pointer<TfLiteRegistration>, int, int)>();

  /// Adds an op registration for a custom operator.
  ///
  /// Op registrations are used to map ops referenced in the flatbuffer model
  /// to executable function pointers (`TfLiteRegistration`s).
  ///
  /// NOTE: The interpreter will make a shallow copy of `registration` internally,
  /// so the caller should ensure that its contents (function pointers, etc...)
  /// remain valid for the duration of any created interpreter's lifetime. A
  /// common practice is making the provided `TfLiteRegistration` instance static.
  ///
  /// The lifetime of the string pointed to by `name` must be at least as long
  /// as the lifetime of the `TfLiteInterpreterOptions`.
  ///
  /// Code that uses this function should NOT call
  /// `TfLiteInterpreterOptionsSetOpResolver` on the same options object.
  ///
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsAddCustomOp(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<ffi.Char> name,
    ffi.Pointer<TfLiteRegistration> registration,
    int min_version,
    int max_version,
  ) {
    return _TfLiteInterpreterOptionsAddCustomOp(
      options,
      name,
      registration,
      min_version,
      max_version,
    );
  }

  late final _TfLiteInterpreterOptionsAddCustomOpPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<TfLiteRegistration>,
              ffi.Int32,
              ffi.Int32)>>('TfLiteInterpreterOptionsAddCustomOp');
  late final _TfLiteInterpreterOptionsAddCustomOp =
      _TfLiteInterpreterOptionsAddCustomOpPtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<TfLiteRegistration>,
              int,
              int)>();

  /// Registers callbacks for resolving builtin or custom operators.
  ///
  /// The `TfLiteInterpreterOptionsSetOpResolver` function provides an alternative
  /// method for registering builtin ops and/or custom ops, by providing operator
  /// resolver callbacks.  Unlike using `TfLiteInterpreterOptionsAddBuiltinOp`
  /// and/or `TfLiteInterpreterOptionsAddAddCustomOp`, these let you register all
  /// the operators in a single call.
  ///
  /// Code that uses this function should NOT call
  /// `TfLiteInterpreterOptionsAddBuiltin` or
  /// `TfLiteInterpreterOptionsAddCustomOp` on the same options object.
  ///
  /// If `op_resolver_user_data` is non-null, its lifetime must be at least as
  /// long as the lifetime of the `TfLiteInterpreterOptions`.
  ///
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsSetOpResolver(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<TfLiteRegistration> Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Int32 op,
                    ffi.Int version)>>
        find_builtin_op,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<TfLiteRegistration> Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<ffi.Char> custom_op,
                    ffi.Int version)>>
        find_custom_op,
    ffi.Pointer<ffi.Void> op_resolver_user_data,
  ) {
    return _TfLiteInterpreterOptionsSetOpResolver(
      options,
      find_builtin_op,
      find_custom_op,
      op_resolver_user_data,
    );
  }

  late final _TfLiteInterpreterOptionsSetOpResolverPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteRegistration> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Int32 op,
                          ffi.Int version)>>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteRegistration> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> custom_op,
                          ffi.Int version)>>,
              ffi.Pointer<ffi.Void>)>>('TfLiteInterpreterOptionsSetOpResolver');
  late final _TfLiteInterpreterOptionsSetOpResolver =
      _TfLiteInterpreterOptionsSetOpResolverPtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteRegistration> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Int32 op,
                          ffi.Int version)>>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteRegistration> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> custom_op,
                          ffi.Int version)>>,
              ffi.Pointer<ffi.Void>)>();

  /// \private
  /// `TfLiteRegistration_V1` version of TfLiteInterpreterOptionsSetOpResolver.
  ///
  /// WARNING: This function is deprecated / not an official part of the API, is
  /// only for binary backwards compatibility, and should not be called.
  void TfLiteInterpreterOptionsSetOpResolverV1(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<TfLiteRegistration_V1> Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Int32 op,
                    ffi.Int version)>>
        find_builtin_op_v1,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<TfLiteRegistration_V1> Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<ffi.Char> op,
                    ffi.Int version)>>
        find_custom_op_v1,
    ffi.Pointer<ffi.Void> op_resolver_user_data,
  ) {
    return _TfLiteInterpreterOptionsSetOpResolverV1(
      options,
      find_builtin_op_v1,
      find_custom_op_v1,
      op_resolver_user_data,
    );
  }

  late final _TfLiteInterpreterOptionsSetOpResolverV1Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Pointer<TfLiteRegistration_V1> Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Int32 op,
                              ffi.Int version)>>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Pointer<TfLiteRegistration_V1> Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<ffi.Char> op,
                              ffi.Int version)>>,
                  ffi.Pointer<ffi.Void>)>>(
      'TfLiteInterpreterOptionsSetOpResolverV1');
  late final _TfLiteInterpreterOptionsSetOpResolverV1 =
      _TfLiteInterpreterOptionsSetOpResolverV1Ptr.asFunction<
          void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteRegistration_V1> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Int32 op,
                          ffi.Int version)>>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteRegistration_V1> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> op,
                          ffi.Int version)>>,
              ffi.Pointer<ffi.Void>)>();

  /// Returns a new interpreter using the provided model and options, or null on
  /// failure, where the model uses only the operators explicitly added to the
  /// options.  This is the same as `TFLiteInterpreterCreate` from `c_api.h`,
  /// except that the only operators that are supported are the ones registered
  /// in `options` via calls to `TfLiteInterpreterOptionsSetOpResolver`,
  /// `TfLiteInterpreterOptionsAddBuiltinOp`, and/or
  /// `TfLiteInterpreterOptionsAddCustomOp`.
  ///
  /// * `model` must be a valid model instance. The caller retains ownership of
  /// the object, and can destroy it immediately after creating the interpreter;
  /// the interpreter will maintain its own reference to the underlying model
  /// data.
  /// * `options` should not be null. The caller retains ownership of the object,
  /// and can safely destroy it immediately after creating the interpreter.
  ///
  /// NOTE: The client *must* explicitly allocate tensors before attempting to
  /// access input tensor data or invoke the interpreter.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<TfLiteInterpreter> TfLiteInterpreterCreateWithSelectedOps(
    ffi.Pointer<TfLiteModel> model,
    ffi.Pointer<TfLiteInterpreterOptions> options,
  ) {
    return _TfLiteInterpreterCreateWithSelectedOps(
      model,
      options,
    );
  }

  late final _TfLiteInterpreterCreateWithSelectedOpsPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteInterpreter> Function(ffi.Pointer<TfLiteModel>,
                  ffi.Pointer<TfLiteInterpreterOptions>)>>(
      'TfLiteInterpreterCreateWithSelectedOps');
  late final _TfLiteInterpreterCreateWithSelectedOps =
      _TfLiteInterpreterCreateWithSelectedOpsPtr.asFunction<
          ffi.Pointer<TfLiteInterpreter> Function(ffi.Pointer<TfLiteModel>,
              ffi.Pointer<TfLiteInterpreterOptions>)>();

  /// Enable or disable the NN API delegate for the interpreter (true to enable).
  ///
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsSetUseNNAPI(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    bool enable,
  ) {
    return _TfLiteInterpreterOptionsSetUseNNAPI(
      options,
      enable,
    );
  }

  late final _TfLiteInterpreterOptionsSetUseNNAPIPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Bool)>>('TfLiteInterpreterOptionsSetUseNNAPI');
  late final _TfLiteInterpreterOptionsSetUseNNAPI =
      _TfLiteInterpreterOptionsSetUseNNAPIPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>, bool)>();

  /// Enable or disable CPU fallback for the interpreter (true to enable).
  /// If enabled, TfLiteInterpreterInvoke will do automatic fallback from
  /// executing with delegate(s) to regular execution without delegates
  /// (i.e. on CPU).
  ///
  /// Allowing the fallback is suitable only if both of the following hold:
  /// - The caller is known not to cache pointers to tensor data across
  /// TfLiteInterpreterInvoke calls.
  /// - The model is not stateful (no variables, no LSTMs) or the state isn't
  /// needed between batches.
  ///
  /// When delegate fallback is enabled, TfLiteInterpreterInvoke will
  /// behave as follows:
  /// If one or more delegates were set in the interpreter options
  /// (see TfLiteInterpreterOptionsAddDelegate),
  /// AND inference fails,
  /// then the interpreter will fall back to not using any delegates.
  /// In that case, the previously applied delegate(s) will be automatically
  /// undone, and an attempt will be made to return the interpreter to an
  /// invokable state, which may invalidate previous tensor addresses,
  /// and the inference will be attempted again, using input tensors with
  /// the same value as previously set.
  ///
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsSetEnableDelegateFallback(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    bool enable,
  ) {
    return _TfLiteInterpreterOptionsSetEnableDelegateFallback(
      options,
      enable,
    );
  }

  late final _TfLiteInterpreterOptionsSetEnableDelegateFallbackPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Bool)>>('TfLiteInterpreterOptionsSetEnableDelegateFallback');
  late final _TfLiteInterpreterOptionsSetEnableDelegateFallback =
      _TfLiteInterpreterOptionsSetEnableDelegateFallbackPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>, bool)>();

  /// Set if buffer handle output is allowed.
  ///
  /// When using hardware delegation, Interpreter will make the data of output
  /// tensors available in `tensor->data` by default. If the application can
  /// consume the buffer handle directly (e.g. reading output from OpenGL
  /// texture), it can set this flag to false, so Interpreter won't copy the
  /// data from buffer handle to CPU memory. WARNING: This is an experimental
  /// API and subject to change.
  void TfLiteSetAllowBufferHandleOutput(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    bool allow_buffer_handle_output,
  ) {
    return _TfLiteSetAllowBufferHandleOutput(
      interpreter,
      allow_buffer_handle_output,
    );
  }

  late final _TfLiteSetAllowBufferHandleOutputPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Bool)>>('TfLiteSetAllowBufferHandleOutput');
  late final _TfLiteSetAllowBufferHandleOutput =
      _TfLiteSetAllowBufferHandleOutputPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreter>, bool)>();

  /// Allow a delegate to look at the graph and modify the graph to handle
  /// parts of the graph themselves. After this is called, the graph may
  /// contain new nodes that replace 1 more nodes.
  /// 'delegate' must outlive the interpreter.
  /// Use `TfLiteInterpreterOptionsAddDelegate` instead of this unless
  /// absolutely required.
  /// Returns one of the following three status codes:
  /// 1. kTfLiteOk: Success.
  /// 2. kTfLiteDelegateError: Delegation failed due to an error in the
  /// delegate. The Interpreter has been restored to its pre-delegation state.
  /// NOTE: This undoes all delegates previously applied to the Interpreter.
  /// 3. kTfLiteError: Unexpected/runtime failure.
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteInterpreterModifyGraphWithDelegate(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TfLiteInterpreterModifyGraphWithDelegate(
      interpreter,
      delegate,
    );
  }

  late final _TfLiteInterpreterModifyGraphWithDelegatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>,
                  ffi.Pointer<TfLiteDelegate>)>>(
      'TfLiteInterpreterModifyGraphWithDelegate');
  late final _TfLiteInterpreterModifyGraphWithDelegate =
      _TfLiteInterpreterModifyGraphWithDelegatePtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteInterpreter>, ffi.Pointer<TfLiteDelegate>)>();

  /// Returns the tensor index corresponding to the input tensor
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteInterpreterGetInputTensorIndex(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int input_index,
  ) {
    return _TfLiteInterpreterGetInputTensorIndex(
      interpreter,
      input_index,
    );
  }

  late final _TfLiteInterpreterGetInputTensorIndexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetInputTensorIndex');
  late final _TfLiteInterpreterGetInputTensorIndex =
      _TfLiteInterpreterGetInputTensorIndexPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Returns the tensor index corresponding to the output tensor
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteInterpreterGetOutputTensorIndex(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int output_index,
  ) {
    return _TfLiteInterpreterGetOutputTensorIndex(
      interpreter,
      output_index,
    );
  }

  late final _TfLiteInterpreterGetOutputTensorIndexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetOutputTensorIndex');
  late final _TfLiteInterpreterGetOutputTensorIndex =
      _TfLiteInterpreterGetOutputTensorIndexPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Returns the number of signatures defined in the model.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteInterpreterGetSignatureCount(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterGetSignatureCount(
      interpreter,
    );
  }

  late final _TfLiteInterpreterGetSignatureCountPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterGetSignatureCount');
  late final _TfLiteInterpreterGetSignatureCount =
      _TfLiteInterpreterGetSignatureCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the key of the Nth signature in the model, where N is specified as
  /// `signature_index`.
  ///
  /// NOTE: The lifetime of the returned key is the same as (and depends on) the
  /// lifetime of `interpreter`.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<ffi.Char> TfLiteInterpreterGetSignatureKey(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int signature_index,
  ) {
    return _TfLiteInterpreterGetSignatureKey(
      interpreter,
      signature_index,
    );
  }

  late final _TfLiteInterpreterGetSignatureKeyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetSignatureKey');
  late final _TfLiteInterpreterGetSignatureKey =
      _TfLiteInterpreterGetSignatureKeyPtr.asFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Returns a new signature runner using the provided interpreter and signature
  /// key, or nullptr on failure.
  ///
  /// NOTE: `signature_key` is a null-terminated C string that must match the
  /// key of a signature in the interpreter's model.
  ///
  /// NOTE: The returned signature runner should be destroyed, by calling
  /// TfLiteSignatureRunnerDelete(), before the interpreter is destroyed.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<TfLiteSignatureRunner> TfLiteInterpreterGetSignatureRunner(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    ffi.Pointer<ffi.Char> signature_key,
  ) {
    return _TfLiteInterpreterGetSignatureRunner(
      interpreter,
      signature_key,
    );
  }

  late final _TfLiteInterpreterGetSignatureRunnerPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteSignatureRunner> Function(
              ffi.Pointer<TfLiteInterpreter>,
              ffi.Pointer<ffi.Char>)>>('TfLiteInterpreterGetSignatureRunner');
  late final _TfLiteInterpreterGetSignatureRunner =
      _TfLiteInterpreterGetSignatureRunnerPtr.asFunction<
          ffi.Pointer<TfLiteSignatureRunner> Function(
              ffi.Pointer<TfLiteInterpreter>, ffi.Pointer<ffi.Char>)>();

  /// Returns the number of inputs associated with a signature.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteSignatureRunnerGetInputCount(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerGetInputCount(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerGetInputCountPtr = _lookup<
          ffi.NativeFunction<
              ffi.Size Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerGetInputCount');
  late final _TfLiteSignatureRunnerGetInputCount =
      _TfLiteSignatureRunnerGetInputCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the (null-terminated) name of the Nth input in a signature, where N
  /// is specified as `input_index`.
  ///
  /// NOTE: The lifetime of the returned name is the same as (and depends on) the
  /// lifetime of `signature_runner`.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<ffi.Char> TfLiteSignatureRunnerGetInputName(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    int input_index,
  ) {
    return _TfLiteSignatureRunnerGetInputName(
      signature_runner,
      input_index,
    );
  }

  late final _TfLiteSignatureRunnerGetInputNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Int32)>>('TfLiteSignatureRunnerGetInputName');
  late final _TfLiteSignatureRunnerGetInputName =
      _TfLiteSignatureRunnerGetInputNamePtr.asFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteSignatureRunner>, int)>();

  /// Resizes the input tensor identified as `input_name` to be the dimensions
  /// specified by `input_dims` and `input_dims_size`. Only unknown dimensions can
  /// be resized with this function. Unknown dimensions are indicated as `-1` in
  /// the `dims_signature` attribute of a TfLiteTensor.
  ///
  /// Returns status of failure or success. Note that this doesn't actually resize
  /// any existing buffers. A call to TfLiteSignatureRunnerAllocateTensors() is
  /// required to change the tensor input buffer.
  ///
  /// NOTE: This function is similar to TfLiteInterpreterResizeInputTensorStrict()
  /// and not TfLiteInterpreterResizeInputTensor().
  ///
  /// NOTE: `input_name` must match the name of an input in the signature.
  ///
  /// NOTE: This function makes a copy of the input dimensions, so the caller can
  /// safely deallocate `input_dims` immediately after this function returns.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteSignatureRunnerResizeInputTensor(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    ffi.Pointer<ffi.Char> input_name,
    ffi.Pointer<ffi.Int> input_dims,
    int input_dims_size,
  ) {
    return _TfLiteSignatureRunnerResizeInputTensor(
      signature_runner,
      input_name,
      input_dims,
      input_dims_size,
    );
  }

  late final _TfLiteSignatureRunnerResizeInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Int>,
              ffi.Int32)>>('TfLiteSignatureRunnerResizeInputTensor');
  late final _TfLiteSignatureRunnerResizeInputTensor =
      _TfLiteSignatureRunnerResizeInputTensorPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Int>, int)>();

  /// Updates allocations for tensors associated with a signature and resizes
  /// dependent tensors using the specified input tensor dimensionality.
  /// This is a relatively expensive operation and hence should only be called
  /// after initializing the signature runner object and/or resizing any inputs.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteSignatureRunnerAllocateTensors(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerAllocateTensors(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerAllocateTensorsPtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerAllocateTensors');
  late final _TfLiteSignatureRunnerAllocateTensors =
      _TfLiteSignatureRunnerAllocateTensorsPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the input tensor identified by `input_name` in the given signature.
  /// Returns nullptr if the given name is not valid.
  ///
  /// NOTE: The lifetime of the returned tensor is the same as (and depends on)
  /// the lifetime of `signature_runner`.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<TfLiteTensor> TfLiteSignatureRunnerGetInputTensor(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    ffi.Pointer<ffi.Char> input_name,
  ) {
    return _TfLiteSignatureRunnerGetInputTensor(
      signature_runner,
      input_name,
    );
  }

  late final _TfLiteSignatureRunnerGetInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>)>>('TfLiteSignatureRunnerGetInputTensor');
  late final _TfLiteSignatureRunnerGetInputTensor =
      _TfLiteSignatureRunnerGetInputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteSignatureRunner>, ffi.Pointer<ffi.Char>)>();

  /// Runs inference on a given signature.
  ///
  /// Before calling this function, the caller should first invoke
  /// TfLiteSignatureRunnerAllocateTensors() and should also set the values for
  /// the input tensors. After successfully calling this function, the values for
  /// the output tensors will be set.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteSignatureRunnerInvoke(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerInvoke(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerInvokePtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerInvoke');
  late final _TfLiteSignatureRunnerInvoke = _TfLiteSignatureRunnerInvokePtr
      .asFunction<int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the number of output tensors associated with the signature.
  ///
  /// WARNING: This is an experimental API and subject to change.
  int TfLiteSignatureRunnerGetOutputCount(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerGetOutputCount(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerGetOutputCountPtr = _lookup<
          ffi.NativeFunction<
              ffi.Size Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerGetOutputCount');
  late final _TfLiteSignatureRunnerGetOutputCount =
      _TfLiteSignatureRunnerGetOutputCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the (null-terminated) name of the Nth output in a signature, where
  /// N is specified as `output_index`.
  ///
  /// NOTE: The lifetime of the returned name is the same as (and depends on) the
  /// lifetime of `signature_runner`.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<ffi.Char> TfLiteSignatureRunnerGetOutputName(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    int output_index,
  ) {
    return _TfLiteSignatureRunnerGetOutputName(
      signature_runner,
      output_index,
    );
  }

  late final _TfLiteSignatureRunnerGetOutputNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Int32)>>('TfLiteSignatureRunnerGetOutputName');
  late final _TfLiteSignatureRunnerGetOutputName =
      _TfLiteSignatureRunnerGetOutputNamePtr.asFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteSignatureRunner>, int)>();

  /// Returns the output tensor identified by `output_name` in the given
  /// signature. Returns nullptr if the given name is not valid.
  ///
  /// NOTE: The lifetime of the returned tensor is the same as (and depends on)
  /// the lifetime of `signature_runner`.
  ///
  /// WARNING: This is an experimental API and subject to change.
  ffi.Pointer<TfLiteTensor> TfLiteSignatureRunnerGetOutputTensor(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    ffi.Pointer<ffi.Char> output_name,
  ) {
    return _TfLiteSignatureRunnerGetOutputTensor(
      signature_runner,
      output_name,
    );
  }

  late final _TfLiteSignatureRunnerGetOutputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>)>>('TfLiteSignatureRunnerGetOutputTensor');
  late final _TfLiteSignatureRunnerGetOutputTensor =
      _TfLiteSignatureRunnerGetOutputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteSignatureRunner>, ffi.Pointer<ffi.Char>)>();

  /// Destroys the signature runner.
  ///
  /// WARNING: This is an experimental API and subject to change.
  void TfLiteSignatureRunnerDelete(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerDelete(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerDelete');
  late final _TfLiteSignatureRunnerDelete = _TfLiteSignatureRunnerDeletePtr
      .asFunction<void Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Return a delegate that uses CoreML for ops execution.
  /// Must outlive the interpreter.
  ffi.Pointer<TfLiteDelegate> TfLiteCoreMlDelegateCreate(
    ffi.Pointer<TfLiteCoreMlDelegateOptions> options,
  ) {
    return _TfLiteCoreMlDelegateCreate(
      options,
    );
  }

  late final _TfLiteCoreMlDelegateCreatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteDelegate> Function(
                  ffi.Pointer<TfLiteCoreMlDelegateOptions>)>>(
      'TfLiteCoreMlDelegateCreate');
  late final _TfLiteCoreMlDelegateCreate =
      _TfLiteCoreMlDelegateCreatePtr.asFunction<
          ffi.Pointer<TfLiteDelegate> Function(
              ffi.Pointer<TfLiteCoreMlDelegateOptions>)>();

  /// Do any needed cleanup and delete 'delegate'.
  void TfLiteCoreMlDelegateDelete(
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TfLiteCoreMlDelegateDelete(
      delegate,
    );
  }

  late final _TfLiteCoreMlDelegateDeletePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteDelegate>)>>(
      'TfLiteCoreMlDelegateDelete');
  late final _TfLiteCoreMlDelegateDelete = _TfLiteCoreMlDelegateDeletePtr
      .asFunction<void Function(ffi.Pointer<TfLiteDelegate>)>();

  /// Populates TFLGpuDelegateOptions as follows:
  /// allow_precision_loss = false;
  /// wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive;
  /// enable_quantization = true;
  TFLGpuDelegateOptions TFLGpuDelegateOptionsDefault() {
    return _TFLGpuDelegateOptionsDefault();
  }

  late final _TFLGpuDelegateOptionsDefaultPtr =
      _lookup<ffi.NativeFunction<TFLGpuDelegateOptions Function()>>(
          'TFLGpuDelegateOptionsDefault');
  late final _TFLGpuDelegateOptionsDefault = _TFLGpuDelegateOptionsDefaultPtr
      .asFunction<TFLGpuDelegateOptions Function()>();

  /// Creates a new delegate instance that need to be destroyed with
  /// `TFLDeleteTfLiteGpuDelegate` when delegate is no longer used by TFLite.
  /// When `options` is set to `nullptr`, the following default values are used:
  /// .precision_loss_allowed = false,
  /// .wait_type = kPassive,
  ffi.Pointer<TfLiteDelegate> TFLGpuDelegateCreate(
    ffi.Pointer<TFLGpuDelegateOptions> options,
  ) {
    return _TFLGpuDelegateCreate(
      options,
    );
  }

  late final _TFLGpuDelegateCreatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteDelegate> Function(
              ffi.Pointer<TFLGpuDelegateOptions>)>>('TFLGpuDelegateCreate');
  late final _TFLGpuDelegateCreate = _TFLGpuDelegateCreatePtr.asFunction<
      ffi.Pointer<TfLiteDelegate> Function(
          ffi.Pointer<TFLGpuDelegateOptions>)>();

  /// Destroys a delegate created with `TFLGpuDelegateCreate` call.
  void TFLGpuDelegateDelete(
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TFLGpuDelegateDelete(
      delegate,
    );
  }

  late final _TFLGpuDelegateDeletePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteDelegate>)>>(
      'TFLGpuDelegateDelete');
  late final _TFLGpuDelegateDelete = _TFLGpuDelegateDeletePtr.asFunction<
      void Function(ffi.Pointer<TfLiteDelegate>)>();

  /// Binds Metal buffer to an input or an output tensor in the initialized
  /// delegate. Bound buffer should have sufficient storage to accommodate all
  /// elements of a tensor. For quantized model, the buffer is bound to internal
  /// dequantized float32 tensor.
  /// Returns non-zero on success, or zero otherwise.
  ///
  /// *** Must be called *after* `Interpreter::ModifyGraphWithDelegate`. ***
  /// WARNING: This is an experimental API and subject to change.
  bool TFLGpuDelegateBindMetalBufferToTensor(
    ffi.Pointer<TfLiteDelegate> delegate,
    int tensor_index,
    int arg2,
  ) {
    return _TFLGpuDelegateBindMetalBufferToTensor(
      delegate,
      tensor_index,
      arg2,
    );
  }

  late final _TFLGpuDelegateBindMetalBufferToTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<TfLiteDelegate>, ffi.Int,
              ffi.Int)>>('TFLGpuDelegateBindMetalBufferToTensor');
  late final _TFLGpuDelegateBindMetalBufferToTensor =
      _TFLGpuDelegateBindMetalBufferToTensorPtr.asFunction<
          bool Function(ffi.Pointer<TfLiteDelegate>, int, int)>();

  /// Returns a structure with the default XNNPack delegate options.
  TfLiteXNNPackDelegateOptions TfLiteXNNPackDelegateOptionsDefault() {
    return _TfLiteXNNPackDelegateOptionsDefault();
  }

  late final _TfLiteXNNPackDelegateOptionsDefaultPtr =
      _lookup<ffi.NativeFunction<TfLiteXNNPackDelegateOptions Function()>>(
          'TfLiteXNNPackDelegateOptionsDefault');
  late final _TfLiteXNNPackDelegateOptionsDefault =
      _TfLiteXNNPackDelegateOptionsDefaultPtr.asFunction<
          TfLiteXNNPackDelegateOptions Function()>();

  /// Creates a new delegate instance that need to be destroyed with
  /// `TfLiteXNNPackDelegateDelete` when delegate is no longer used by TFLite.
  /// When `options` is set to `nullptr`, default values are used (see
  /// implementation of TfLiteXNNPackDelegateOptionsDefault in the .cc file for
  /// details).
  ffi.Pointer<TfLiteDelegate> TfLiteXNNPackDelegateCreate(
    ffi.Pointer<TfLiteXNNPackDelegateOptions> options,
  ) {
    return _TfLiteXNNPackDelegateCreate(
      options,
    );
  }

  late final _TfLiteXNNPackDelegateCreatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteDelegate> Function(
                  ffi.Pointer<TfLiteXNNPackDelegateOptions>)>>(
      'TfLiteXNNPackDelegateCreate');
  late final _TfLiteXNNPackDelegateCreate =
      _TfLiteXNNPackDelegateCreatePtr.asFunction<
          ffi.Pointer<TfLiteDelegate> Function(
              ffi.Pointer<TfLiteXNNPackDelegateOptions>)>();

  /// Performs the same task as TfLiteXNNPackDelegateCreate, with one exception.
  /// If the context passed contains a non-null xnnpack_threadpool field,
  /// we will use it as the threadpool for the delegate created.
  ffi.Pointer<TfLiteDelegate> TfLiteXNNPackDelegateCreateWithThreadpool(
    ffi.Pointer<TfLiteXNNPackDelegateOptions> options,
    ffi.Pointer<TfLiteContext> context,
  ) {
    return _TfLiteXNNPackDelegateCreateWithThreadpool(
      options,
      context,
    );
  }

  late final _TfLiteXNNPackDelegateCreateWithThreadpoolPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteDelegate> Function(
                  ffi.Pointer<TfLiteXNNPackDelegateOptions>,
                  ffi.Pointer<TfLiteContext>)>>(
      'TfLiteXNNPackDelegateCreateWithThreadpool');
  late final _TfLiteXNNPackDelegateCreateWithThreadpool =
      _TfLiteXNNPackDelegateCreateWithThreadpoolPtr.asFunction<
          ffi.Pointer<TfLiteDelegate> Function(
              ffi.Pointer<TfLiteXNNPackDelegateOptions>,
              ffi.Pointer<TfLiteContext>)>();

  /// Returns the pthreadpool_t object used for parallelization in XNNPACK.
  /// Can return NULL if the XNNPack delegate is single-threaded.
  ///
  /// WARNING: This API is experimental and subject to change.
  ffi.Pointer<ffi.Void> TfLiteXNNPackDelegateGetThreadPool(
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TfLiteXNNPackDelegateGetThreadPool(
      delegate,
    );
  }

  late final _TfLiteXNNPackDelegateGetThreadPoolPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteDelegate>)>>(
      'TfLiteXNNPackDelegateGetThreadPool');
  late final _TfLiteXNNPackDelegateGetThreadPool =
      _TfLiteXNNPackDelegateGetThreadPoolPtr.asFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteDelegate>)>();

  /// Destroys a delegate created with `TfLiteXNNPackDelegateCreate` call.
  void TfLiteXNNPackDelegateDelete(
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TfLiteXNNPackDelegateDelete(
      delegate,
    );
  }

  late final _TfLiteXNNPackDelegateDeletePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteDelegate>)>>(
      'TfLiteXNNPackDelegateDelete');
  late final _TfLiteXNNPackDelegateDelete = _TfLiteXNNPackDelegateDeletePtr
      .asFunction<void Function(ffi.Pointer<TfLiteDelegate>)>();

  /// Creates a new weights cache that can be shared with multiple delegate
  /// instances. Prefer TfLiteXNNPackDelegateWeightsCacheCreateWithSize which can
  /// reduce memory bandwidth.
  ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>
      TfLiteXNNPackDelegateWeightsCacheCreate() {
    return _TfLiteXNNPackDelegateWeightsCacheCreate();
  }

  late final _TfLiteXNNPackDelegateWeightsCacheCreatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>
              Function()>>('TfLiteXNNPackDelegateWeightsCacheCreate');
  late final _TfLiteXNNPackDelegateWeightsCacheCreate =
      _TfLiteXNNPackDelegateWeightsCacheCreatePtr.asFunction<
          ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> Function()>();

  /// Creates a new weights cache with a specified initial size that can be shared
  /// with multiple delegate instances. The weights cache can hold up to size bytes
  /// without growing.
  ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>
      TfLiteXNNPackDelegateWeightsCacheCreateWithSize(
    int size,
  ) {
    return _TfLiteXNNPackDelegateWeightsCacheCreateWithSize(
      size,
    );
  }

  late final _TfLiteXNNPackDelegateWeightsCacheCreateWithSizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> Function(
              ffi.Size)>>('TfLiteXNNPackDelegateWeightsCacheCreateWithSize');
  late final _TfLiteXNNPackDelegateWeightsCacheCreateWithSize =
      _TfLiteXNNPackDelegateWeightsCacheCreateWithSizePtr.asFunction<
          ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> Function(int)>();

  /// Soft-finalize a weights cache. Extra space will be left in the weights cache
  /// to allow for cache "insertion" only if it is a cache hit. This has memory
  /// overhead compared to TfLiteXNNPackDelegateWeightsCacheFinalizeHard. Use this
  /// if the number of interpreter instances using XNNPACK delegate is not fixed
  /// (e.g. created based on workload in a server daemon).
  /// Returns true on success, false on error.
  bool TfLiteXNNPackDelegateWeightsCacheFinalizeSoft(
    ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> cache,
  ) {
    return _TfLiteXNNPackDelegateWeightsCacheFinalizeSoft(
      cache,
    );
  }

  late final _TfLiteXNNPackDelegateWeightsCacheFinalizeSoftPtr = _lookup<
          ffi.NativeFunction<
              ffi.Bool Function(
                  ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>)>>(
      'TfLiteXNNPackDelegateWeightsCacheFinalizeSoft');
  late final _TfLiteXNNPackDelegateWeightsCacheFinalizeSoft =
      _TfLiteXNNPackDelegateWeightsCacheFinalizeSoftPtr.asFunction<
          bool Function(ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>)>();

  /// Hard-finalize a weights cache, cache is effectively frozen and no more cache
  /// operations are allowed. Memory is resized to smallest possible. Use this if
  /// the number of interpreter instances using XNNPACK delegate can be fixed and
  /// all creation of instances can happen up front. This has the lowest memory
  /// usage.
  /// Returns true on success, false on error.
  bool TfLiteXNNPackDelegateWeightsCacheFinalizeHard(
    ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> cache,
  ) {
    return _TfLiteXNNPackDelegateWeightsCacheFinalizeHard(
      cache,
    );
  }

  late final _TfLiteXNNPackDelegateWeightsCacheFinalizeHardPtr = _lookup<
          ffi.NativeFunction<
              ffi.Bool Function(
                  ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>)>>(
      'TfLiteXNNPackDelegateWeightsCacheFinalizeHard');
  late final _TfLiteXNNPackDelegateWeightsCacheFinalizeHard =
      _TfLiteXNNPackDelegateWeightsCacheFinalizeHardPtr.asFunction<
          bool Function(ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>)>();

  /// Destroys a weights cache created with
  /// `TfLiteXNNPackDelegateWeightsCacheCreate` call.
  void TfLiteXNNPackDelegateWeightsCacheDelete(
    ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> cache,
  ) {
    return _TfLiteXNNPackDelegateWeightsCacheDelete(
      cache,
    );
  }

  late final _TfLiteXNNPackDelegateWeightsCacheDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>)>>(
      'TfLiteXNNPackDelegateWeightsCacheDelete');
  late final _TfLiteXNNPackDelegateWeightsCacheDelete =
      _TfLiteXNNPackDelegateWeightsCacheDeletePtr.asFunction<
          void Function(ffi.Pointer<TfLiteXNNPackDelegateWeightsCache>)>();

  /// Populates TfLiteGpuDelegateOptionsV2 as follows:
  /// is_precision_loss_allowed = false
  /// inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER
  /// priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION
  /// priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO
  /// priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO
  /// experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT
  /// max_delegated_partitions = 1
  TfLiteGpuDelegateOptionsV2 TfLiteGpuDelegateOptionsV2Default() {
    return _TfLiteGpuDelegateOptionsV2Default();
  }

  late final _TfLiteGpuDelegateOptionsV2DefaultPtr =
      _lookup<ffi.NativeFunction<TfLiteGpuDelegateOptionsV2 Function()>>(
          'TfLiteGpuDelegateOptionsV2Default');
  late final _TfLiteGpuDelegateOptionsV2Default =
      _TfLiteGpuDelegateOptionsV2DefaultPtr.asFunction<
          TfLiteGpuDelegateOptionsV2 Function()>();

  /// Creates a new delegate instance that need to be destroyed with
  /// TfLiteGpuDelegateV2Delete when delegate is no longer used by TFLite.
  ///
  /// This delegate encapsulates multiple GPU-acceleration APIs under the hood to
  /// make use of the fastest available on a device.
  ///
  /// When `options` is set to `nullptr`, then default options are used.
  ffi.Pointer<TfLiteDelegate> TfLiteGpuDelegateV2Create(
    ffi.Pointer<TfLiteGpuDelegateOptionsV2> options,
  ) {
    return _TfLiteGpuDelegateV2Create(
      options,
    );
  }

  late final _TfLiteGpuDelegateV2CreatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteDelegate> Function(
                  ffi.Pointer<TfLiteGpuDelegateOptionsV2>)>>(
      'TfLiteGpuDelegateV2Create');
  late final _TfLiteGpuDelegateV2Create =
      _TfLiteGpuDelegateV2CreatePtr.asFunction<
          ffi.Pointer<TfLiteDelegate> Function(
              ffi.Pointer<TfLiteGpuDelegateOptionsV2>)>();

  /// Destroys a delegate created with `TfLiteGpuDelegateV2Create` call.
  void TfLiteGpuDelegateV2Delete(
    ffi.Pointer<TfLiteDelegate> delegate,
  ) {
    return _TfLiteGpuDelegateV2Delete(
      delegate,
    );
  }

  late final _TfLiteGpuDelegateV2DeletePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteDelegate>)>>(
      'TfLiteGpuDelegateV2Delete');
  late final _TfLiteGpuDelegateV2Delete = _TfLiteGpuDelegateV2DeletePtr
      .asFunction<void Function(ffi.Pointer<TfLiteDelegate>)>();
}

class TfLiteModel extends ffi.Opaque {}

class TfLiteInterpreterOptions extends ffi.Opaque {}

/// WARNING: This is an experimental interface that is subject to change.
class TfLiteDelegate extends ffi.Struct {
  /// Data that delegate needs to identify itself. This data is owned by the
  /// delegate. The delegate is owned in the user code, so the delegate is
  /// responsible for deallocating this when it is destroyed.
  external ffi.Pointer<ffi.Void> data_;

  /// Invoked by ModifyGraphWithDelegate. This prepare is called, giving the
  /// delegate a view of the current graph through TfLiteContext*. It typically
  /// will look at the nodes and call ReplaceNodeSubsetsWithDelegateKernels()
  /// to ask the TensorFlow lite runtime to create macro-nodes to represent
  /// delegated subgraphs of the original graph.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate)>> Prepare;

  /// Copy the data from delegate buffer handle into raw memory of the given
  /// 'tensor'. Note that the delegate is allowed to allocate the raw bytes as
  /// long as it follows the rules for kTfLiteDynamic tensors, in which case this
  /// cannot be null.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteTensor> tensor)>> CopyFromBufferHandle;

  /// Copy the data from raw memory of the given 'tensor' to delegate buffer
  /// handle. This can be null if the delegate doesn't use its own buffer.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteTensor> tensor)>> CopyToBufferHandle;

  /// Free the Delegate Buffer Handle. Note: This only frees the handle, but
  /// this doesn't release the underlying resource (e.g. textures). The
  /// resources are either owned by application layer or the delegate.
  /// This can be null if the delegate doesn't use its own buffer.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate,
              ffi.Pointer<TfLiteBufferHandle> handle)>> FreeBufferHandle;

  /// Bitmask flags. See the comments in `TfLiteDelegateFlags`.
  @ffi.Int64()
  external int flags;

  /// The opaque delegate builder associated with this object.  If set then the
  /// TF Lite runtime will give precedence to this field.  E.g. instead of
  /// invoking 'Prepare' via the function pointer inside the 'TfLiteDelegate'
  /// object, the runtime will first check if the corresponding function
  /// pointer inside 'opaque_delegate_builder' is set and if so invoke that.
  ///
  /// If this field is non-null, then the 'Prepare' field (of the
  /// 'TfLiteDelegate') should be null.
  external ffi.Pointer<TfLiteOpaqueDelegateBuilder> opaque_delegate_builder;
}

/// Note that new error status values may be added in future in order to
/// indicate more fine-grained internal states, therefore, applications should
/// not rely on status values being members of the enum.
abstract class TfLiteStatus {
  static const int kTfLiteOk = 0;

  /// Generally referring to an error in the runtime (i.e. interpreter)
  static const int kTfLiteError = 1;

  /// Generally referring to an error from a TfLiteDelegate itself.
  static const int kTfLiteDelegateError = 2;

  /// Generally referring to an error in applying a delegate due to
  /// incompatibility between runtime and delegate, e.g., this error is returned
  /// when trying to apply a TF Lite delegate onto a model graph that's already
  /// immutable.
  static const int kTfLiteApplicationError = 3;

  /// Generally referring to serialized delegate data not being found.
  /// See tflite::delegates::Serialization.
  static const int kTfLiteDelegateDataNotFound = 4;

  /// Generally referring to data-writing issues in delegate serialization.
  /// See tflite::delegates::Serialization.
  static const int kTfLiteDelegateDataWriteError = 5;

  /// Generally referring to data-reading issues in delegate serialization.
  /// See tflite::delegates::Serialization.
  static const int kTfLiteDelegateDataReadError = 6;

  /// Generally referring to issues when the TF Lite model has ops that cannot be
  /// resolved at runtime. This could happen when the specific op is not
  /// registered or built with the TF Lite framework.
  static const int kTfLiteUnresolvedOps = 7;

  /// Generally referring to invocation cancelled by the user.
  /// See `interpreter::Cancel`.
  /// TODO(b/194915839): Implement `interpreter::Cancel`.
  /// TODO(b/250636993): Cancellation triggered by `SetCancellationFunction`
  /// should also return this status code.
  static const int kTfLiteCancelled = 8;
}

/// Forward declare so dependent structs and methods can reference these types
/// prior to the struct definitions.
class TfLiteContext extends ffi.Struct {
  /// Number of tensors in the context.
  @ffi.Size()
  external int tensors_size;

  /// The execution plan contains a list of the node indices in execution
  /// order. execution_plan->size is the current number of nodes. And,
  /// execution_plan->data[0] is the first node that needs to be run.
  /// TfLiteDelegates can traverse the current execution plan by iterating
  /// through each member of this array and using GetNodeAndRegistration() to
  /// access details about a node. i.e.
  ///
  /// TfLiteIntArray* execution_plan;
  /// TF_LITE_ENSURE_STATUS(context->GetExecutionPlan(context, &execution_plan));
  /// for (int exec_index = 0; exec_index < execution_plan->size; exec_index++) {
  /// int node_index = execution_plan->data[exec_index];
  /// TfLiteNode* node;
  /// TfLiteRegistration* reg;
  /// context->GetNodeAndRegistration(context, node_index, &node, &reg);
  /// }
  /// Note: the memory pointed by '`*execution_plan` is OWNED by TfLite runtime.
  /// Future calls to GetExecutionPlan invalidates earlier outputs. The following
  /// code snippet shows the issue of such an invocation pattern. After calling
  /// CheckNode, subsequent access to `plan_1st` is undefined.
  ///
  /// void CheckNode(const TfLiteNode* node) {
  /// ...
  /// TfLiteIntArray* plan_2nd;
  /// TF_LITE_ENSURE_STATUS(context->GetExecutionPlan(context, &plan_2nd));
  /// ...
  /// }
  ///
  /// TfLiteIntArray* plan_1st;
  /// TF_LITE_ENSURE_STATUS(context->GetExecutionPlan(context, &plan_1st));
  /// for (int exec_index = 0; exec_index < plan_1st->size; exec_index++) {
  /// int node_index = plan_1st->data[exec_index];
  /// TfLiteNode* node;
  /// TfLiteRegistration* reg;
  /// context->GetNodeAndRegistration(context, node_index, &node, &reg);
  /// CheckNode(node);
  /// }
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<TfLiteContext> context,
                  ffi.Pointer<ffi.Pointer<TfLiteIntArray>> execution_plan)>>
      GetExecutionPlan;

  /// An array of tensors in the interpreter context (of length `tensors_size`)
  external ffi.Pointer<TfLiteTensor> tensors;

  /// opaque full context ptr (an opaque c++ data structure)
  external ffi.Pointer<ffi.Void> impl_;

  /// Request memory pointer be resized. Updates dimensions on the tensor.
  /// NOTE: ResizeTensor takes ownership of newSize.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteContext>,
              ffi.Pointer<TfLiteTensor>,
              ffi.Pointer<TfLiteIntArray>)>> ResizeTensor;

  /// Request that an error be reported with format string msg.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteContext>, ffi.Pointer<ffi.Char>)>> ReportError;

  /// Add `tensors_to_add` tensors, preserving pre-existing Tensor entries.  If
  /// non-null, the value pointed to by `first_new_tensor_index` will be set to
  /// the index of the first new tensor.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Int32 Function(
                  ffi.Pointer<TfLiteContext>, ffi.Int, ffi.Pointer<ffi.Int>)>>
      AddTensors;

  /// Get a Tensor node by node_index.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Int32 Function(
                  ffi.Pointer<TfLiteContext>,
                  ffi.Int,
                  ffi.Pointer<ffi.Pointer<TfLiteNode>>,
                  ffi.Pointer<ffi.Pointer<TfLiteRegistration>>)>>
      GetNodeAndRegistration;

  /// Replace ops with one or more stub delegate operations. This function
  /// does not take ownership of `nodes_to_replace`.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<TfLiteContext>, TfLiteRegistration,
                  ffi.Pointer<TfLiteIntArray>, ffi.Pointer<TfLiteDelegate>)>>
      ReplaceNodeSubsetsWithDelegateKernels;

  /// Number of threads that are recommended to subsystems like gemmlowp and
  /// eigen.
  @ffi.Int()
  external int recommended_num_threads;

  /// Access external contexts by type.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteExternalContext> Function(
              ffi.Pointer<TfLiteContext>, ffi.Int32)>> GetExternalContext;

  /// Set the value of a external context. Does not take ownership of the
  /// pointer.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext>, ffi.Int32,
              ffi.Pointer<TfLiteExternalContext>)>> SetExternalContext;

  /// Flag for allowing float16 precision for FP32 calculation.
  /// default: false.
  /// WARNING: This is an experimental API and subject to change.
  @ffi.Bool()
  external bool allow_fp32_relax_to_fp16;

  /// Pointer to the op-level profiler, if set; nullptr otherwise.
  external ffi.Pointer<ffi.Void> profiler;

  /// Allocate persistent buffer which has the same life time as the interpreter.
  /// Returns nullptr on failure.
  /// The memory is allocated from heap for TFL, and from tail in TFLM.
  /// This method is only available in Init or Prepare stage.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Void> Function(
                  ffi.Pointer<TfLiteContext> ctx, ffi.Size bytes)>>
      AllocatePersistentBuffer;

  /// Allocate a buffer which will be deallocated right after invoke phase.
  /// The memory is allocated from heap in TFL, and from volatile arena in TFLM.
  /// This method is only available in invoke stage.
  /// NOTE: If possible use RequestScratchBufferInArena method to avoid memory
  /// allocation during inference time.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> ctx, ffi.Size bytes,
              ffi.Pointer<ffi.Pointer<ffi.Void>> ptr)>> AllocateBufferForEval;

  /// Request a scratch buffer in the arena through static memory planning.
  /// This method is only available in Prepare stage and the buffer is allocated
  /// by the interpreter between Prepare and Eval stage. In Eval stage,
  /// GetScratchBuffer API can be used to fetch the address.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> ctx, ffi.Size bytes,
              ffi.Pointer<ffi.Int> buffer_idx)>> RequestScratchBufferInArena;

  /// Get the scratch buffer pointer.
  /// This method is only available in Eval stage.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Void> Function(
                  ffi.Pointer<TfLiteContext> ctx, ffi.Int buffer_idx)>>
      GetScratchBuffer;

  /// Resize the memory pointer of the `tensor`. This method behaves the same as
  /// `ResizeTensor`, except that it makes a copy of the shape array internally
  /// so the shape array could be deallocated right afterwards.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteContext> ctx,
              ffi.Pointer<TfLiteTensor> tensor,
              ffi.Int dims,
              ffi.Pointer<ffi.Int> shape)>> ResizeTensorExplicit;

  /// This method provides a preview of post-delegation partitioning. Each
  /// TfLiteDelegateParams in the referenced array corresponds to one instance of
  /// the delegate kernel.
  /// Example usage:
  ///
  /// TfLiteIntArray* nodes_to_replace = ...;
  /// TfLiteDelegateParams* params_array;
  /// int num_partitions = 0;
  /// TF_LITE_ENSURE_STATUS(context->PreviewDelegatePartitioning(
  /// context, delegate, nodes_to_replace, &params_array, &num_partitions));
  /// for (int idx = 0; idx < num_partitions; idx++) {
  /// const auto& partition_params = params_array[idx];
  /// ...
  /// }
  ///
  /// NOTE: The context owns the memory referenced by partition_params_array. It
  /// will be cleared with another call to PreviewDelegateParitioning, or after
  /// TfLiteDelegateParams::Prepare returns.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Int32 Function(
                  ffi.Pointer<TfLiteContext> context,
                  ffi.Pointer<TfLiteIntArray> nodes_to_replace,
                  ffi.Pointer<ffi.Pointer<TfLiteDelegateParams>>
                      partition_params_array,
                  ffi.Pointer<ffi.Int> num_partitions)>>
      PreviewDelegatePartitioning;

  /// Returns a TfLiteTensor struct for a given index.
  /// WARNING: This is an experimental interface that is subject to change.
  /// WARNING: This method may not be available on all platforms.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteTensor> Function(
                  ffi.Pointer<TfLiteContext> context, ffi.Int tensor_idx)>>
      GetTensor;

  /// Returns a TfLiteEvalTensor struct for a given index.
  /// WARNING: This is an experimental interface that is subject to change.
  /// WARNING: This method may not be available on all platforms.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteEvalTensor> Function(
                  ffi.Pointer<TfLiteContext> context, ffi.Int tensor_idx)>>
      GetEvalTensor;

  /// Retrieves named metadata buffer from the TFLite model.
  /// Returns kTfLiteOk if metadata is successfully obtained from the flatbuffer
  /// Model: that is, there exists a `metadata` entry with given `name` string.
  /// (see TFLite's schema.fbs).
  /// The corresponding `buffer` information is populated in `ptr` & `bytes`.
  /// The data from `ptr` is valid for the lifetime of the Interpreter.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> name,
              ffi.Pointer<ffi.Pointer<ffi.Char>> ptr,
              ffi.Pointer<ffi.Size> bytes)>> GetModelMetadata;
}

/// Fixed size list of integers. Used for dimensions and inputs/outputs tensor
/// indices
class TfLiteIntArray extends ffi.Opaque {}

class TfLiteTensor extends ffi.Struct {
  /// The data type specification for data stored in `data`. This affects
  /// what member of `data` union should be used.
  @ffi.Int32()
  external int type;

  /// A union of data pointers. The appropriate type should be used for a typed
  /// tensor based on `type`.
  external TfLitePtrUnion data;

  /// A pointer to a structure representing the dimensionality interpretation
  /// that the buffer should have. NOTE: the product of elements of `dims`
  /// and the element datatype size should be equal to `bytes` below.
  external ffi.Pointer<TfLiteIntArray> dims;

  /// Quantization information.
  external TfLiteQuantizationParams params;

  /// How memory is mapped
  /// kTfLiteMmapRo: Memory mapped read only.
  /// i.e. weights
  /// kTfLiteArenaRw: Arena allocated read write memory
  /// (i.e. temporaries, outputs).
  @ffi.Int32()
  external int allocation_type;

  /// The number of bytes required to store the data of this Tensor. I.e.
  /// (bytes of each element) * dims[0] * ... * dims[n-1].  For example, if
  /// type is kTfLiteFloat32 and dims = {3, 2} then
  /// bytes = sizeof(float) * 3 * 2 = 4 * 3 * 2 = 24.
  @ffi.Size()
  external int bytes;

  /// An opaque pointer to a tflite::MMapAllocation
  external ffi.Pointer<ffi.Void> allocation;

  /// Null-terminated name of this tensor.
  external ffi.Pointer<ffi.Char> name;

  /// The delegate which knows how to handle `buffer_handle`.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<TfLiteDelegate> delegate;

  /// An integer buffer handle that can be handled by `delegate`.
  /// The value is valid only when delegate is not null.
  /// WARNING: This is an experimental interface that is subject to change.
  @TfLiteBufferHandle()
  external int buffer_handle;

  /// If the delegate uses its own buffer (e.g. GPU memory), the delegate is
  /// responsible to set data_is_stale to true.
  /// `delegate->CopyFromBufferHandle` can be called to copy the data from
  /// delegate buffer.
  /// WARNING: This is an // experimental interface that is subject to change.
  @ffi.Bool()
  external bool data_is_stale;

  /// True if the tensor is a variable.
  @ffi.Bool()
  external bool is_variable;

  /// Quantization information. Replaces params field above.
  external TfLiteQuantization quantization;

  /// Parameters used to encode a sparse tensor.
  /// This is optional. The field is NULL if a tensor is dense.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<TfLiteSparsity> sparsity;

  /// Optional. Encodes shapes with unknown dimensions with -1. This field is
  /// only populated when unknown dimensions exist in a read-write tensor (i.e.
  /// an input or output tensor). (e.g.  `dims` contains [1, 1, 1, 3] and
  /// `dims_signature` contains [1, -1, -1, 3]). Note that this field only
  /// exists when TF_LITE_STATIC_MEMORY is not defined.
  external ffi.Pointer<TfLiteIntArray> dims_signature;
}

/// Types supported by tensor
abstract class TfLiteType {
  static const int kTfLiteNoType = 0;
  static const int kTfLiteFloat32 = 1;
  static const int kTfLiteInt32 = 2;
  static const int kTfLiteUInt8 = 3;
  static const int kTfLiteInt64 = 4;
  static const int kTfLiteString = 5;
  static const int kTfLiteBool = 6;
  static const int kTfLiteInt16 = 7;
  static const int kTfLiteComplex64 = 8;
  static const int kTfLiteInt8 = 9;
  static const int kTfLiteFloat16 = 10;
  static const int kTfLiteFloat64 = 11;
  static const int kTfLiteComplex128 = 12;
  static const int kTfLiteUInt64 = 13;
  static const int kTfLiteResource = 14;
  static const int kTfLiteVariant = 15;
  static const int kTfLiteUInt32 = 16;
  static const int kTfLiteUInt16 = 17;
  static const int kTfLiteInt4 = 18;
}

/// A union of pointers that points to memory for a given tensor.
class TfLitePtrUnion extends ffi.Union {
  /// Do not access these members directly, if possible, use
  /// GetTensorData<TYPE>(tensor) instead, otherwise only access .data, as other
  /// members are deprecated.
  external ffi.Pointer<ffi.Int32> i32;

  external ffi.Pointer<ffi.Uint32> u32;

  external ffi.Pointer<ffi.Int64> i64;

  external ffi.Pointer<ffi.Uint64> u64;

  external ffi.Pointer<ffi.Float> f;

  external ffi.Pointer<TfLiteFloat16> f16;

  external ffi.Pointer<ffi.Double> f64;

  external ffi.Pointer<ffi.Char> raw;

  external ffi.Pointer<ffi.Char> raw_const;

  external ffi.Pointer<ffi.Uint8> uint8;

  external ffi.Pointer<ffi.Bool> b;

  external ffi.Pointer<ffi.Int16> i16;

  external ffi.Pointer<ffi.Uint16> ui16;

  external ffi.Pointer<TfLiteComplex64> c64;

  external ffi.Pointer<TfLiteComplex128> c128;

  external ffi.Pointer<ffi.Int8> int8;

  /// Only use this member.
  external ffi.Pointer<ffi.Void> data;
}

/// Half precision data type compatible with the C99 definition.
class TfLiteFloat16 extends ffi.Struct {
  @ffi.Uint16()
  external int data;
}

/// Single-precision complex data type compatible with the C99 definition.
class TfLiteComplex64 extends ffi.Struct {
  /// real and imaginary parts, respectively.
  @ffi.Float()
  external double re;

  @ffi.Float()
  external double im;
}

/// Double-precision complex data type compatible with the C99 definition.
class TfLiteComplex128 extends ffi.Struct {
  /// real and imaginary parts, respectively.
  @ffi.Double()
  external double re;

  @ffi.Double()
  external double im;
}

/// Legacy. Will be deprecated in favor of TfLiteAffineQuantization.
/// If per-layer quantization is specified this field will still be populated in
/// addition to TfLiteAffineQuantization.
/// Parameters for asymmetric quantization. Quantized values can be converted
/// back to float using:
/// real_value = scale * (quantized_value - zero_point)
class TfLiteQuantizationParams extends ffi.Struct {
  @ffi.Float()
  external double scale;

  @ffi.Int32()
  external int zero_point;
}

/// Memory allocation strategies.
/// * kTfLiteMmapRo: Read-only memory-mapped data, or data externally allocated.
/// * kTfLiteArenaRw: Arena allocated with no guarantees about persistence,
/// and available during eval.
/// * kTfLiteArenaRwPersistent: Arena allocated but persistent across eval, and
/// only available during eval.
/// * kTfLiteDynamic: Allocated during eval, or for string tensors.
/// * kTfLitePersistentRo: Allocated and populated during prepare. This is
/// useful for tensors that can be computed during prepare and treated
/// as constant inputs for downstream ops (also in prepare).
/// * kTfLiteCustom: Custom memory allocation provided by the user. See
/// TfLiteCustomAllocation below.
abstract class TfLiteAllocationType {
  static const int kTfLiteMemNone = 0;
  static const int kTfLiteMmapRo = 1;
  static const int kTfLiteArenaRw = 2;
  static const int kTfLiteArenaRwPersistent = 3;
  static const int kTfLiteDynamic = 4;
  static const int kTfLitePersistentRo = 5;
  static const int kTfLiteCustom = 6;
}

/// The delegates should use zero or positive integers to represent handles.
/// -1 is reserved from unallocated status.
typedef TfLiteBufferHandle = ffi.Int;

/// Structure specifying the quantization used by the tensor, if-any.
class TfLiteQuantization extends ffi.Struct {
  /// The type of quantization held by params.
  @ffi.Int32()
  external int type;

  /// Holds an optional reference to a quantization param structure. The actual
  /// type depends on the value of the `type` field (see the comment there for
  /// the values and corresponding types).
  external ffi.Pointer<ffi.Void> params;
}

/// SupportedQuantizationTypes.
abstract class TfLiteQuantizationType {
  /// No quantization.
  static const int kTfLiteNoQuantization = 0;

  /// Affine quantization (with support for per-channel quantization).
  /// Corresponds to TfLiteAffineQuantization.
  static const int kTfLiteAffineQuantization = 1;
}

/// Parameters used to encode a sparse tensor. For detailed explanation of each
/// field please refer to lite/schema/schema.fbs.
class TfLiteSparsity extends ffi.Struct {
  external ffi.Pointer<TfLiteIntArray> traversal_order;

  external ffi.Pointer<TfLiteIntArray> block_map;

  external ffi.Pointer<TfLiteDimensionMetadata> dim_metadata;

  @ffi.Int()
  external int dim_metadata_size;
}

/// Metadata to encode each dimension in a sparse tensor.
class TfLiteDimensionMetadata extends ffi.Struct {
  @ffi.Int32()
  external int format;

  @ffi.Int()
  external int dense_size;

  external ffi.Pointer<TfLiteIntArray> array_segments;

  external ffi.Pointer<TfLiteIntArray> array_indices;
}

/// Storage format of each dimension in a sparse tensor.
abstract class TfLiteDimensionType {
  static const int kTfLiteDimDense = 0;
  static const int kTfLiteDimSparseCSR = 1;
}

/// A structure representing an instance of a node.
/// This structure only exhibits the inputs, outputs, user defined data and some
/// node properties (like statefulness), not other features like the type.
class TfLiteNode extends ffi.Struct {
  /// Inputs to this node expressed as indices into the simulator's tensors.
  external ffi.Pointer<TfLiteIntArray> inputs;

  /// Outputs to this node expressed as indices into the simulator's tensors.
  external ffi.Pointer<TfLiteIntArray> outputs;

  /// intermediate tensors to this node expressed as indices into the simulator's
  /// tensors.
  external ffi.Pointer<TfLiteIntArray> intermediates;

  /// Temporary tensors uses during the computations. This usually contains no
  /// tensors, but ops are allowed to change that if they need scratch space of
  /// any sort.
  external ffi.Pointer<TfLiteIntArray> temporaries;

  /// Opaque data provided by the node implementer through `Registration.init`.
  external ffi.Pointer<ffi.Void> user_data;

  /// Opaque data provided to the node if the node is a builtin. This is usually
  /// a structure defined in builtin_op_data.h
  external ffi.Pointer<ffi.Void> builtin_data;

  /// Custom initial data. This is the opaque data provided in the flatbuffer.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<ffi.Void> custom_initial_data;

  @ffi.Int()
  external int custom_initial_data_size;

  /// The pointer to the delegate. This is non-null only when the node is
  /// created by calling `interpreter.ModifyGraphWithDelegate`.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<TfLiteDelegate> delegate;

  /// Whether this op might have side effect (e.g. stateful op).
  @ffi.Bool()
  external bool might_have_side_effect;
}

class TfLiteRegistration extends ffi.Struct {
  /// Initializes the op from serialized data.
  /// Called only *once* for the lifetime of the op, so any one-time allocations
  /// should be made here (unless they depend on tensor sizes).
  ///
  /// If a built-in op:
  /// `buffer` is the op's params data (TfLiteLSTMParams*).
  /// `length` is zero.
  /// If custom op:
  /// `buffer` is the op's `custom_options`.
  /// `length` is the size of the buffer.
  ///
  /// Returns a type-punned (i.e. void*) opaque data (e.g. a primitive pointer
  /// or an instance of a struct).
  ///
  /// The returned pointer will be stored with the node in the `user_data` field,
  /// accessible within prepare and invoke functions below.
  /// NOTE: if the data is already in the desired format, simply implement this
  /// function to return `nullptr` and implement the free function to be a no-op.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> buffer, ffi.Size length)>> init;

  /// The pointer `buffer` is the data previously returned by an init invocation.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Void> buffer)>> free;

  /// prepare is called when the inputs this node depends on have been resized.
  /// context->ResizeTensor() can be called to request output tensors to be
  /// resized.
  /// Can be called multiple times for the lifetime of the op.
  ///
  /// Returns kTfLiteOk on success.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> prepare;

  /// Execute the node (should read node->inputs and output to node->outputs).
  /// Returns kTfLiteOk on success.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> invoke;

  /// profiling_string is called during summarization of profiling information
  /// in order to group executions together. Providing a value here will cause a
  /// given op to appear multiple times is the profiling report. This is
  /// particularly useful for custom ops that can perform significantly
  /// different calculations depending on their `user-data`.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> profiling_string;

  /// Builtin codes. If this kernel refers to a builtin this is the code
  /// of the builtin. This is so we can do marshaling to other frameworks like
  /// NN API.
  /// Note: It is the responsibility of the registration binder to set this
  /// properly.
  @ffi.Int32()
  external int builtin_code;

  /// Custom op name. If the op is a builtin, this will be null.
  /// Note: It is the responsibility of the registration binder to set this
  /// properly.
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<ffi.Char> custom_name;

  /// The version of the op.
  /// Note: It is the responsibility of the registration binder to set this
  /// properly.
  @ffi.Int()
  external int version;

  /// The external version of `TfLiteRegistration`. Since we can't use internal
  /// types (such as `TfLiteContext`) for C API to maintain ABI stability.
  /// C API user will provide `TfLiteRegistrationExternal` to implement custom
  /// ops. We keep it inside of `TfLiteRegistration` and use it to route
  /// callbacks properly.
  external ffi.Pointer<TfLiteRegistrationExternal> registration_external;
}

class TfLiteRegistrationExternal extends ffi.Opaque {}

/// An external context is a collection of information unrelated to the TF Lite
/// framework, but useful to a subset of the ops. TF Lite knows very little
/// about the actual contexts, but it keeps a list of them, and is able to
/// refresh them if configurations like the number of recommended threads
/// change.
class TfLiteExternalContext extends ffi.Struct {
  @ffi.Int32()
  external int type;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> context)>> Refresh;
}

/// The list of external context types known to TF Lite. This list exists solely
/// to avoid conflicts and to ensure ops can share the external contexts they
/// need. Access to the external contexts is controlled by one of the
/// corresponding support files.
abstract class TfLiteExternalContextType {
  /// include eigen_support.h to use.
  static const int kTfLiteEigenContext = 0;

  /// include gemm_support.h to use.
  static const int kTfLiteGemmLowpContext = 1;

  /// Placeholder for Edge TPU support.
  static const int kTfLiteEdgeTpuContext = 2;

  /// include cpu_backend_context.h to use.
  static const int kTfLiteCpuBackendContext = 3;
  static const int kTfLiteMaxExternalContexts = 4;
}

/// WARNING: This is an experimental interface that is subject to change.
///
/// Currently, TfLiteDelegateParams has to be allocated in a way that it's
/// trivially destructable. It will be stored as `builtin_data` field in
/// `TfLiteNode` of the delegate node.
///
/// See also the `CreateDelegateParams` function in `interpreter.cc` details.
class TfLiteDelegateParams extends ffi.Struct {
  external ffi.Pointer<TfLiteDelegate> delegate;

  external ffi.Pointer<TfLiteIntArray> nodes_to_replace;

  external ffi.Pointer<TfLiteIntArray> input_tensors;

  external ffi.Pointer<TfLiteIntArray> output_tensors;
}

/// Light-weight tensor struct for TF Micro runtime. Provides the minimal amount
/// of information required for a kernel to run during TfLiteRegistration::Eval.
/// TODO(b/160955687): Move this field into TF_LITE_STATIC_MEMORY when TFLM
/// builds with this flag by default internally.
class TfLiteEvalTensor extends ffi.Struct {
  /// A union of data pointers. The appropriate type should be used for a typed
  /// tensor based on `type`.
  external TfLitePtrUnion data;

  /// A pointer to a structure representing the dimensionality interpretation
  /// that the buffer should have.
  external ffi.Pointer<TfLiteIntArray> dims;

  /// The data type specification for data stored in `data`. This affects
  /// what member of `data` union should be used.
  @ffi.Int32()
  external int type;
}

/// `TfLiteOpaqueDelegateBuilder` is used for constructing
/// `TfLiteOpaqueDelegateStruct`, see `TfLiteOpaqueDelegateCreate` below.  Note:
/// This struct is not ABI stable.
///
/// For forward source compatibility `TfLiteOpaqueDelegateBuilder` objects should
/// be brace-initialized, so that all fields (including any that might be added
/// in the future) get zero-initialized.  The purpose of each field is exactly
/// the same as with `TfLiteDelegate`.
///
/// WARNING: This is an experimental interface that is subject to change.
class TfLiteOpaqueDelegateBuilder extends ffi.Struct {
  /// Data that delegate needs to identify itself. This data is owned by the
  /// delegate. The delegate is owned in the user code, so the delegate is
  /// responsible for deallocating this when it is destroyed.
  external ffi.Pointer<ffi.Void> data;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegateStruct> delegate,
              ffi.Pointer<ffi.Void> data)>> Prepare;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegateStruct> delegate,
              ffi.Pointer<ffi.Void> data,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteOpaqueTensor> tensor)>> CopyFromBufferHandle;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegateStruct> delegate,
              ffi.Pointer<ffi.Void> data,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteOpaqueTensor> tensor)>> CopyToBufferHandle;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegateStruct> delegate,
              ffi.Pointer<ffi.Void> data,
              ffi.Pointer<TfLiteBufferHandle> handle)>> FreeBufferHandle;

  /// Bitmask flags. See the comments in `TfLiteDelegateFlags`.
  @ffi.Int64()
  external int flags;
}

class TfLiteOpaqueContext extends ffi.Opaque {}

class TfLiteOpaqueDelegateStruct extends ffi.Opaque {}

class TfLiteOpaqueTensor extends ffi.Opaque {}

class TfLiteInterpreter extends ffi.Opaque {}

/// The enum for builtin operators.
/// Note: CUSTOM, DELEGATE, and PLACEHOLDER_FOR_GREATER_OP_CODES are 3 special
/// ops which are not real built-in ops.
abstract class TfLiteBuiltinOperator {
  static const int kTfLiteBuiltinAdd = 0;
  static const int kTfLiteBuiltinAveragePool2d = 1;
  static const int kTfLiteBuiltinConcatenation = 2;
  static const int kTfLiteBuiltinConv2d = 3;
  static const int kTfLiteBuiltinDepthwiseConv2d = 4;
  static const int kTfLiteBuiltinDepthToSpace = 5;
  static const int kTfLiteBuiltinDequantize = 6;
  static const int kTfLiteBuiltinEmbeddingLookup = 7;
  static const int kTfLiteBuiltinFloor = 8;
  static const int kTfLiteBuiltinFullyConnected = 9;
  static const int kTfLiteBuiltinHashtableLookup = 10;
  static const int kTfLiteBuiltinL2Normalization = 11;
  static const int kTfLiteBuiltinL2Pool2d = 12;
  static const int kTfLiteBuiltinLocalResponseNormalization = 13;
  static const int kTfLiteBuiltinLogistic = 14;
  static const int kTfLiteBuiltinLshProjection = 15;
  static const int kTfLiteBuiltinLstm = 16;
  static const int kTfLiteBuiltinMaxPool2d = 17;
  static const int kTfLiteBuiltinMul = 18;
  static const int kTfLiteBuiltinRelu = 19;
  static const int kTfLiteBuiltinReluN1To1 = 20;
  static const int kTfLiteBuiltinRelu6 = 21;
  static const int kTfLiteBuiltinReshape = 22;
  static const int kTfLiteBuiltinResizeBilinear = 23;
  static const int kTfLiteBuiltinRnn = 24;
  static const int kTfLiteBuiltinSoftmax = 25;
  static const int kTfLiteBuiltinSpaceToDepth = 26;
  static const int kTfLiteBuiltinSvdf = 27;
  static const int kTfLiteBuiltinTanh = 28;
  static const int kTfLiteBuiltinConcatEmbeddings = 29;
  static const int kTfLiteBuiltinSkipGram = 30;
  static const int kTfLiteBuiltinCall = 31;
  static const int kTfLiteBuiltinCustom = 32;
  static const int kTfLiteBuiltinEmbeddingLookupSparse = 33;
  static const int kTfLiteBuiltinPad = 34;
  static const int kTfLiteBuiltinUnidirectionalSequenceRnn = 35;
  static const int kTfLiteBuiltinGather = 36;
  static const int kTfLiteBuiltinBatchToSpaceNd = 37;
  static const int kTfLiteBuiltinSpaceToBatchNd = 38;
  static const int kTfLiteBuiltinTranspose = 39;
  static const int kTfLiteBuiltinMean = 40;
  static const int kTfLiteBuiltinSub = 41;
  static const int kTfLiteBuiltinDiv = 42;
  static const int kTfLiteBuiltinSqueeze = 43;
  static const int kTfLiteBuiltinUnidirectionalSequenceLstm = 44;
  static const int kTfLiteBuiltinStridedSlice = 45;
  static const int kTfLiteBuiltinBidirectionalSequenceRnn = 46;
  static const int kTfLiteBuiltinExp = 47;
  static const int kTfLiteBuiltinTopkV2 = 48;
  static const int kTfLiteBuiltinSplit = 49;
  static const int kTfLiteBuiltinLogSoftmax = 50;
  static const int kTfLiteBuiltinDelegate = 51;
  static const int kTfLiteBuiltinBidirectionalSequenceLstm = 52;
  static const int kTfLiteBuiltinCast = 53;
  static const int kTfLiteBuiltinPrelu = 54;
  static const int kTfLiteBuiltinMaximum = 55;
  static const int kTfLiteBuiltinArgMax = 56;
  static const int kTfLiteBuiltinMinimum = 57;
  static const int kTfLiteBuiltinLess = 58;
  static const int kTfLiteBuiltinNeg = 59;
  static const int kTfLiteBuiltinPadv2 = 60;
  static const int kTfLiteBuiltinGreater = 61;
  static const int kTfLiteBuiltinGreaterEqual = 62;
  static const int kTfLiteBuiltinLessEqual = 63;
  static const int kTfLiteBuiltinSelect = 64;
  static const int kTfLiteBuiltinSlice = 65;
  static const int kTfLiteBuiltinSin = 66;
  static const int kTfLiteBuiltinTransposeConv = 67;
  static const int kTfLiteBuiltinSparseToDense = 68;
  static const int kTfLiteBuiltinTile = 69;
  static const int kTfLiteBuiltinExpandDims = 70;
  static const int kTfLiteBuiltinEqual = 71;
  static const int kTfLiteBuiltinNotEqual = 72;
  static const int kTfLiteBuiltinLog = 73;
  static const int kTfLiteBuiltinSum = 74;
  static const int kTfLiteBuiltinSqrt = 75;
  static const int kTfLiteBuiltinRsqrt = 76;
  static const int kTfLiteBuiltinShape = 77;
  static const int kTfLiteBuiltinPow = 78;
  static const int kTfLiteBuiltinArgMin = 79;
  static const int kTfLiteBuiltinFakeQuant = 80;
  static const int kTfLiteBuiltinReduceProd = 81;
  static const int kTfLiteBuiltinReduceMax = 82;
  static const int kTfLiteBuiltinPack = 83;
  static const int kTfLiteBuiltinLogicalOr = 84;
  static const int kTfLiteBuiltinOneHot = 85;
  static const int kTfLiteBuiltinLogicalAnd = 86;
  static const int kTfLiteBuiltinLogicalNot = 87;
  static const int kTfLiteBuiltinUnpack = 88;
  static const int kTfLiteBuiltinReduceMin = 89;
  static const int kTfLiteBuiltinFloorDiv = 90;
  static const int kTfLiteBuiltinReduceAny = 91;
  static const int kTfLiteBuiltinSquare = 92;
  static const int kTfLiteBuiltinZerosLike = 93;
  static const int kTfLiteBuiltinFill = 94;
  static const int kTfLiteBuiltinFloorMod = 95;
  static const int kTfLiteBuiltinRange = 96;
  static const int kTfLiteBuiltinResizeNearestNeighbor = 97;
  static const int kTfLiteBuiltinLeakyRelu = 98;
  static const int kTfLiteBuiltinSquaredDifference = 99;
  static const int kTfLiteBuiltinMirrorPad = 100;
  static const int kTfLiteBuiltinAbs = 101;
  static const int kTfLiteBuiltinSplitV = 102;
  static const int kTfLiteBuiltinUnique = 103;
  static const int kTfLiteBuiltinCeil = 104;
  static const int kTfLiteBuiltinReverseV2 = 105;
  static const int kTfLiteBuiltinAddN = 106;
  static const int kTfLiteBuiltinGatherNd = 107;
  static const int kTfLiteBuiltinCos = 108;
  static const int kTfLiteBuiltinWhere = 109;
  static const int kTfLiteBuiltinRank = 110;
  static const int kTfLiteBuiltinElu = 111;
  static const int kTfLiteBuiltinReverseSequence = 112;
  static const int kTfLiteBuiltinMatrixDiag = 113;
  static const int kTfLiteBuiltinQuantize = 114;
  static const int kTfLiteBuiltinMatrixSetDiag = 115;
  static const int kTfLiteBuiltinRound = 116;
  static const int kTfLiteBuiltinHardSwish = 117;
  static const int kTfLiteBuiltinIf = 118;
  static const int kTfLiteBuiltinWhile = 119;
  static const int kTfLiteBuiltinNonMaxSuppressionV4 = 120;
  static const int kTfLiteBuiltinNonMaxSuppressionV5 = 121;
  static const int kTfLiteBuiltinScatterNd = 122;
  static const int kTfLiteBuiltinSelectV2 = 123;
  static const int kTfLiteBuiltinDensify = 124;
  static const int kTfLiteBuiltinSegmentSum = 125;
  static const int kTfLiteBuiltinBatchMatmul = 126;
  static const int kTfLiteBuiltinPlaceholderForGreaterOpCodes = 127;
  static const int kTfLiteBuiltinCumsum = 128;
  static const int kTfLiteBuiltinCallOnce = 129;
  static const int kTfLiteBuiltinBroadcastTo = 130;
  static const int kTfLiteBuiltinRfft2d = 131;
  static const int kTfLiteBuiltinConv3d = 132;
  static const int kTfLiteBuiltinImag = 133;
  static const int kTfLiteBuiltinReal = 134;
  static const int kTfLiteBuiltinComplexAbs = 135;
  static const int kTfLiteBuiltinHashtable = 136;
  static const int kTfLiteBuiltinHashtableFind = 137;
  static const int kTfLiteBuiltinHashtableImport = 138;
  static const int kTfLiteBuiltinHashtableSize = 139;
  static const int kTfLiteBuiltinReduceAll = 140;
  static const int kTfLiteBuiltinConv3dTranspose = 141;
  static const int kTfLiteBuiltinVarHandle = 142;
  static const int kTfLiteBuiltinReadVariable = 143;
  static const int kTfLiteBuiltinAssignVariable = 144;
  static const int kTfLiteBuiltinBroadcastArgs = 145;
  static const int kTfLiteBuiltinRandomStandardNormal = 146;
  static const int kTfLiteBuiltinBucketize = 147;
  static const int kTfLiteBuiltinRandomUniform = 148;
  static const int kTfLiteBuiltinMultinomial = 149;
  static const int kTfLiteBuiltinGelu = 150;
  static const int kTfLiteBuiltinDynamicUpdateSlice = 151;
  static const int kTfLiteBuiltinRelu0To1 = 152;
  static const int kTfLiteBuiltinUnsortedSegmentProd = 153;
  static const int kTfLiteBuiltinUnsortedSegmentMax = 154;
  static const int kTfLiteBuiltinUnsortedSegmentSum = 155;
  static const int kTfLiteBuiltinAtan2 = 156;
  static const int kTfLiteBuiltinUnsortedSegmentMin = 157;
  static const int kTfLiteBuiltinSign = 158;
}

class TfLiteOpaqueNode extends ffi.Opaque {}

typedef va_list = __builtin_va_list;
typedef __builtin_va_list = ffi.Pointer<ffi.Char>;

class TfLiteSignatureRunner extends ffi.Opaque {}

/// Old version of `TfLiteRegistration` to maintain binary backward
/// compatibility.
/// WARNING: This structure is deprecated / not an official part of the API.
/// It should be only used for binary backward compatibility.
class TfLiteRegistration_V1 extends ffi.Struct {
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> buffer, ffi.Size length)>> init;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Void> buffer)>> free;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> prepare;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> invoke;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> profiling_string;

  @ffi.Int32()
  external int builtin_code;

  external ffi.Pointer<ffi.Char> custom_name;

  @ffi.Int()
  external int version;
}

abstract class TfLiteCoreMlDelegateEnabledDevices {
  /// Create Core ML delegate only on devices with Apple Neural Engine.
  /// Returns nullptr otherwise.
  static const int TfLiteCoreMlDelegateDevicesWithNeuralEngine = 0;

  /// Always create Core ML delegate
  static const int TfLiteCoreMlDelegateAllDevices = 1;
}

class TfLiteCoreMlDelegateOptions extends ffi.Struct {
  /// Only create delegate when Neural Engine is available on the device.
  @ffi.Int32()
  external int enabled_devices;

  /// Specifies target Core ML version for model conversion.
  /// Core ML 3 come with a lot more ops, but some ops (e.g. reshape) is not
  /// delegated due to input rank constraint.
  /// if not set to one of the valid versions, the delegate will use highest
  /// version possible in the platform.
  /// Valid versions: (2, 3)
  @ffi.Int()
  external int coreml_version;

  /// This sets the maximum number of Core ML delegates created.
  /// Each graph corresponds to one delegated node subset in the
  /// TFLite model. Set this to 0 to delegate all possible partitions.
  @ffi.Int()
  external int max_delegated_partitions;

  /// This sets the minimum number of nodes per partition delegated with
  /// Core ML delegate. Defaults to 2.
  @ffi.Int()
  external int min_nodes_per_partition;
}

abstract class TFLGpuDelegateWaitType {
  /// waitUntilCompleted
  static const int TFLGpuDelegateWaitTypePassive = 0;

  /// Minimize latency. It uses active spinning instead of mutex and consumes
  /// additional CPU resources.
  static const int TFLGpuDelegateWaitTypeActive = 1;

  /// Useful when the output is used with GPU pipeline then or if external
  /// command encoder is set.
  static const int TFLGpuDelegateWaitTypeDoNotWait = 2;

  /// Tries to avoid GPU sleep mode.
  static const int TFLGpuDelegateWaitTypeAggressive = 3;
}

/// Creates a new delegate instance that need to be destroyed with
/// DeleteFlowDelegate when delegate is no longer used by tflite.
class TFLGpuDelegateOptions extends ffi.Struct {
  /// Allows to quantify tensors, downcast values, process in float16 etc.
  @ffi.Bool()
  external bool allow_precision_loss;

  @ffi.Int32()
  external int wait_type;

  /// Allows execution of integer quantized models
  @ffi.Bool()
  external bool enable_quantization;
}

class TfLiteXNNPackDelegateWeightsCache extends ffi.Opaque {}

class TfLiteXNNPackDelegateOptions extends ffi.Struct {
  /// Number of threads to use in the thread pool.
  /// 0 or negative value means no thread pool used.
  @ffi.Int32()
  external int num_threads;

  /// Bitfield with any combination of the following binary options:
  /// - TFLITE_XNNPACK_DELEGATE_FLAG_QS8
  /// - TFLITE_XNNPACK_DELEGATE_FLAG_QU8
  /// - TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16
  /// - TFLITE_XNNPACK_DELEGATE_FLAG_DYNAMIC_FULLY_CONNECTED
  @ffi.Uint32()
  external int flags;

  /// Cache for packed weights, can be shared between multiple instances of
  /// delegates.
  external ffi.Pointer<TfLiteXNNPackDelegateWeightsCache> weights_cache;

  /// Whether READ_VARIABLE, ASSIGN_VARIABLE, and VARIABLE_HANDLE operations
  /// should be handled by XNNPACK.
  @ffi.Bool()
  external bool handle_variable_ops;
}

/// Encapsulated compilation/runtime tradeoffs.
abstract class TfLiteGpuInferenceUsage {
  /// Delegate will be used only once, therefore, bootstrap/init time should
  /// be taken into account.
  static const int TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER = 0;

  /// Prefer maximizing the throughput. Same delegate will be used repeatedly on
  /// multiple inputs.
  static const int TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED = 1;
}

abstract class TfLiteGpuInferencePriority {
  /// AUTO priority is needed when a single priority is the most important
  /// factor. For example,
  /// priority1 = MIN_LATENCY would result in the configuration that achieves
  /// maximum performance.
  static const int TFLITE_GPU_INFERENCE_PRIORITY_AUTO = 0;
  static const int TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION = 1;
  static const int TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY = 2;
  static const int TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE = 3;
}

/// Used to toggle experimental flags used in the delegate. Note that this is a
/// bitmask, so the values should be 1, 2, 4, 8, ...etc.
abstract class TfLiteGpuExperimentalFlags {
  static const int TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE = 0;

  /// Enables inference on quantized models with the delegate.
  /// NOTE: This is enabled in TfLiteGpuDelegateOptionsV2Default.
  static const int TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT = 1;

  /// Enforces execution with the provided backend.
  static const int TFLITE_GPU_EXPERIMENTAL_FLAGS_CL_ONLY = 2;
  static const int TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY = 4;

  /// Enable serialization of GPU kernels & model data. Speeds up initilization
  /// at the cost of space on disk.
  /// Delegate performs serialization the first time it is applied with a new
  /// model or inference params. Later initializations are fast.
  /// ModifyGraphWithDelegate will fail if data cannot be serialized.
  ///
  /// NOTE: User also needs to set serialization_dir & model_token in
  /// TfLiteGpuDelegateOptionsV2.
  /// Currently works only if CL backend is used.
  static const int TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_SERIALIZATION = 8;
}

/// IMPORTANT: Always use TfLiteGpuDelegateOptionsV2Default() method to create
/// new instance of TfLiteGpuDelegateOptionsV2, otherwise every new added option
/// may break inference.
class TfLiteGpuDelegateOptionsV2 extends ffi.Struct {
  /// When set to zero, computations are carried out in maximal possible
  /// precision. Otherwise, the GPU may quantify tensors, downcast values,
  /// process in FP16 to increase performance. For most models precision loss is
  /// warranted.
  /// [OBSOLETE]: to be removed
  @ffi.Int32()
  external int is_precision_loss_allowed;

  /// Preference is defined in TfLiteGpuInferenceUsage.
  @ffi.Int32()
  external int inference_preference;

  /// Ordered priorities provide better control over desired semantics,
  /// where priority(n) is more important than priority(n+1), therefore,
  /// each time inference engine needs to make a decision, it uses
  /// ordered priorities to do so.
  /// For example:
  /// MAX_PRECISION at priority1 would not allow to decrease precision,
  /// but moving it to priority2 or priority3 would result in F16 calculation.
  ///
  /// Priority is defined in TfLiteGpuInferencePriority.
  /// AUTO priority can only be used when higher priorities are fully specified.
  /// For example:
  /// VALID:   priority1 = MIN_LATENCY, priority2 = AUTO, priority3 = AUTO
  /// VALID:   priority1 = MIN_LATENCY, priority2 = MAX_PRECISION,
  /// priority3 = AUTO
  /// INVALID: priority1 = AUTO, priority2 = MIN_LATENCY, priority3 = AUTO
  /// INVALID: priority1 = MIN_LATENCY, priority2 = AUTO,
  /// priority3 = MAX_PRECISION
  /// Invalid priorities will result in error.
  @ffi.Int32()
  external int inference_priority1;

  @ffi.Int32()
  external int inference_priority2;

  @ffi.Int32()
  external int inference_priority3;

  /// Bitmask flags. See the comments in TfLiteGpuExperimentalFlags.
  @ffi.Int64()
  external int experimental_flags;

  /// A graph could have multiple partitions that can be delegated to the GPU.
  /// This limits the maximum number of partitions to be delegated. By default,
  /// it's set to 1 in TfLiteGpuDelegateOptionsV2Default().
  @ffi.Int32()
  external int max_delegated_partitions;

  /// The nul-terminated directory to use for serialization.
  /// Whether serialization actually happens or not is dependent on backend used
  /// and validity of this directory.
  /// Set to nullptr in TfLiteGpuDelegateOptionsV2Default(), which implies the
  /// delegate will not try serialization.
  ///
  /// NOTE: Users should ensure that this directory is private to the app to
  /// avoid data access issues.
  external ffi.Pointer<ffi.Char> serialization_dir;

  /// The unique nul-terminated token string that acts as a 'namespace' for
  /// all serialization entries.
  /// Should be unique to a particular model (graph & constants).
  /// For an example of how to generate this from a TFLite model, see
  /// StrFingerprint() in lite/delegates/serialization.h.
  ///
  /// Set to nullptr in TfLiteGpuDelegateOptionsV2Default(), which implies the
  /// delegate will not try serialization.
  external ffi.Pointer<ffi.Char> model_token;
}

const int TFLITE_XNNPACK_DELEGATE_FLAG_QS8 = 1;

const int TFLITE_XNNPACK_DELEGATE_FLAG_QU8 = 2;

const int TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16 = 4;

const int TFLITE_XNNPACK_DELEGATE_FLAG_DYNAMIC_FULLY_CONNECTED = 8;
